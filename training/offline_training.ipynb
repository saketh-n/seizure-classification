{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tusz_subset\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_binary_classifier(nb_classes, channels=64, samples=128) -> Model:\n",
    "    input_layer = Input(shape=(channels, samples, 1))\n",
    "    flatten = Flatten(name='flatten')(input_layer)\n",
    "    hidden = Dense(nb_classes, name='hidden')(flatten)\n",
    "    output = Dense(nb_classes, name='output')(hidden)\n",
    "    softmax = Activation('softmax', name='softmax')(output)\n",
    "    return Model(inputs=input_layer, outputs=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_dev, y_dev) = tfds.as_numpy(tfds.load('tusz_subset',\n",
    "                                                             split=['train', 'dev'],\n",
    "                                                             batch_size=-1,\n",
    "                                                             as_supervised=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a925c5e41cd142b0b3a69a6efbc1dfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=45226.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "************ Summary ************\n",
      "\n",
      "Examples/sec (First included) 22035496.13 ex/sec (total: 1447232 ex, 0.07 sec)\n",
      "Examples/sec (First only) 4184647.28 ex/sec (total: 32 ex, 0.00 sec)\n",
      "Examples/sec (First excluded) 22037574.80 ex/sec (total: 1447200 ex, 0.07 sec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>BenchmarkResult:</strong><br/><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>first+lasts</th>\n",
       "      <td>0.065677</td>\n",
       "      <td>1447232</td>\n",
       "      <td>2.203550e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>32</td>\n",
       "      <td>4.184647e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasts</th>\n",
       "      <td>0.065670</td>\n",
       "      <td>1447200</td>\n",
       "      <td>2.203757e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "BenchmarkResult(stats=             duration  num_examples           avg\n",
       "first+lasts  0.065677       1447232  2.203550e+07\n",
       "first        0.000008            32  4.184647e+06\n",
       "lasts        0.065670       1447200  2.203757e+07, raw_stats=                      duration\n",
       "start_time         4902.519755\n",
       "first_batch_time   4902.519762\n",
       "end_time           4902.585432\n",
       "num_iter          45226.000000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.benchmark(x_dev, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9666, 64, 128, 1)\n",
      "[0 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for item in zip(x_train, y_train):\n",
    "    print(np.shape(x_train))\n",
    "    print(y_train)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_binary_classifier(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/6\n",
      "303/303 [==============================] - 2s 8ms/step - loss: 0.4644 - sparse_categorical_accuracy: 0.8227 - val_loss: 0.4999 - val_sparse_categorical_accuracy: 0.8219\n",
      "Epoch 2/6\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.4642 - sparse_categorical_accuracy: 0.8226 - val_loss: 0.5010 - val_sparse_categorical_accuracy: 0.8219\n",
      "Epoch 3/6\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.4640 - sparse_categorical_accuracy: 0.8226 - val_loss: 0.5022 - val_sparse_categorical_accuracy: 0.8219\n",
      "Epoch 4/6\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.4636 - sparse_categorical_accuracy: 0.8226 - val_loss: 0.5029 - val_sparse_categorical_accuracy: 0.8218\n",
      "Epoch 5/6\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.4635 - sparse_categorical_accuracy: 0.8225 - val_loss: 0.5049 - val_sparse_categorical_accuracy: 0.8218\n",
      "Epoch 6/6\n",
      "303/303 [==============================] - 1s 4ms/step - loss: 0.4631 - sparse_categorical_accuracy: 0.8225 - val_loss: 0.5071 - val_sparse_categorical_accuracy: 0.8218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4644293189048767,\n",
       "  0.4641708433628082,\n",
       "  0.4639703333377838,\n",
       "  0.4636498689651489,\n",
       "  0.463479220867157,\n",
       "  0.46313977241516113],\n",
       " 'sparse_categorical_accuracy': [0.8226774334907532,\n",
       "  0.8225739598274231,\n",
       "  0.8225739598274231,\n",
       "  0.8225739598274231,\n",
       "  0.822470486164093,\n",
       "  0.822470486164093],\n",
       " 'val_loss': [0.4998953938484192,\n",
       "  0.5009856224060059,\n",
       "  0.502238929271698,\n",
       "  0.5028719305992126,\n",
       "  0.5049048066139221,\n",
       "  0.5070930123329163],\n",
       " 'val_sparse_categorical_accuracy': [0.8219165802001953,\n",
       "  0.821872353553772,\n",
       "  0.8218502402305603,\n",
       "  0.8218281269073486,\n",
       "  0.821806013584137,\n",
       "  0.8217618465423584]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Fit model on training data')\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=6,\n",
    "    validation_data=(x_dev, y_dev),\n",
    ")\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "354/354 [==============================] - 1s 2ms/step - loss: 0.5071 - sparse_categorical_accuracy: 0.8218\n",
      "test loss, test acc: [0.5070934295654297, 0.8217618465423584]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_dev, y_dev, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/robert/Desktop/School/Grad/481c/seizure-classification/training/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/robert/Desktop/School/Grad/481c/seizure-classification/training/assets\n"
     ]
    }
   ],
   "source": [
    "# Save model weights!\n",
    "import os\n",
    "model.save(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8225739706186633"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (sum(y_train) / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model learns nothing!!\n",
    "# Try to add conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (64, 128, 1)\n",
    "def cnn_binary_classifier(nb_classes, channels=64, samples=128) -> Model:\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(nb_classes))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train conv model\n",
    "cnn_model = cnn_binary_classifier(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/6\n",
      "303/303 [==============================] - 67s 221ms/step - loss: 0.4966 - sparse_categorical_accuracy: 0.8217 - val_loss: 0.4634 - val_sparse_categorical_accuracy: 0.8252\n",
      "Epoch 2/6\n",
      "303/303 [==============================] - 63s 209ms/step - loss: 0.4694 - sparse_categorical_accuracy: 0.8226 - val_loss: 0.4665 - val_sparse_categorical_accuracy: 0.8252\n",
      "Epoch 3/6\n",
      "303/303 [==============================] - 69s 227ms/step - loss: 0.4687 - sparse_categorical_accuracy: 0.8226 - val_loss: 0.4634 - val_sparse_categorical_accuracy: 0.8252\n",
      "Epoch 4/6\n",
      "302/303 [============================>.] - ETA: 0s - loss: 0.4683 - sparse_categorical_accuracy: 0.8225"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-0c4eabdd2bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fit model on training data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = cnn_model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1214\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1215\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Fit model on training data')\n",
    "history = cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=6,\n",
    "    validation_data=(x_dev, y_dev),\n",
    ")\n",
    "history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SpatialDropout2D\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "\n",
    "def EEGNet(nb_classes, channels=64, samples=128,\n",
    "           dropout_rate=0.5, kernel_length=64, f1=8,\n",
    "           d=2, f2=16, norm_rate=0.25, dropout_type='Dropout'):\n",
    "    \"\"\" Keras Implementation of EEGNet\n",
    "    http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta\n",
    "    Note that this implements the newest version of EEGNet and NOT the earlier\n",
    "    version (version v1 and v2 on arxiv). We strongly recommend using this\n",
    "    architecture as it performs much better and has nicer properties than\n",
    "    our earlier version. For example:\n",
    "\n",
    "        1. Depth-wise Convolutions to learn spatial filters within a\n",
    "        temporal convolution. The use of the depth_multiplier option maps\n",
    "        exactly to the number of spatial filters learned within a temporal\n",
    "        filter. This matches the setup of algorithms like FBCSP which learn\n",
    "        spatial filters within each filter in a filter-bank. This also limits\n",
    "        the number of free parameters to fit when compared to a fully-connected\n",
    "        convolution.\n",
    "\n",
    "        2. Separable Convolutions to learn how to optimally combine spatial\n",
    "        filters across temporal bands. Separable Convolutions are Depth-wise\n",
    "        Convolutions followed by (1x1) Point-wise Convolutions.\n",
    "\n",
    "\n",
    "    While the original paper used Dropout, we found that SpatialDropout2D\n",
    "    sometimes produced slightly better results for classification of ERP\n",
    "    signals. However, SpatialDropout2D significantly reduced performance\n",
    "    on the Oscillatory dataset (SMR, BCI-IV Dataset 2A). We recommend using\n",
    "    the default Dropout in most cases.\n",
    "\n",
    "    Assumes the input signal is sampled at 128Hz. If you want to use this model\n",
    "    for any other sampling rate you will need to modify the lengths of temporal\n",
    "    kernels and average pooling size in blocks 1 and 2 as needed (double the\n",
    "    kernel lengths for double the sampling rate, etc). Note that we haven't\n",
    "    tested the model performance with this rule so this may not work well.\n",
    "\n",
    "    The model with default parameters gives the EEGNet-8,2 model as discussed\n",
    "    in the paper. This model should do pretty well in general, although it is\n",
    "    advised to do some model searching to get optimal performance on your\n",
    "    particular dataset. We set f2 = f1 * d (number of input filters = number of output filters)\n",
    "    for the SeparableConv2D layer. We haven't extensively tested other values of this\n",
    "    parameter (say, f2 < f1 * d for compressed learning, and f2 > f1 * d for\n",
    "    overcomplete). We believe the main parameters to focus on are f1 and d.\n",
    "    Inputs:\n",
    "\n",
    "      nb_classes      : int, number of classes to classify\n",
    "      channels, samples  : number of channels and time points in the EEG data\n",
    "      dropout_rate     : dropout fraction\n",
    "      kernel_length      : length of temporal convolution in first layer. We found\n",
    "                        that setting this to be half the sampling rate worked\n",
    "                        well in practice. For the SMR dataset in particular\n",
    "                        since the data was high-passed at 4Hz we used a kernel\n",
    "                        length of 32.\n",
    "      f1, f2          : number of temporal filters (f1) and number of point-wise\n",
    "                        filters (f2) to learn. Default: f1 = 8, f2 = f1 * d.\n",
    "      d               : number of spatial filters to learn within each temporal\n",
    "                        convolution. Default: d = 2\n",
    "      dropout_type     : Either SpatialDropout2D or Dropout, passed as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    if dropout_type == 'SpatialDropout2D':\n",
    "        dropout_type = SpatialDropout2D\n",
    "    elif dropout_type == 'Dropout':\n",
    "        dropout_type = Dropout\n",
    "    else:\n",
    "        raise ValueError('dropout_type must be one of SpatialDropout2D '\n",
    "                         'or Dropout, passed as a string.')\n",
    "\n",
    "    input1 = Input(shape=(channels, samples, 1))\n",
    "\n",
    "    ##################################################################\n",
    "    block1 = Conv2D(f1, (1, kernel_length), padding='same',\n",
    "                    input_shape=(channels, samples, 1),\n",
    "                    use_bias=False)(input1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = DepthwiseConv2D((channels, 1), use_bias=False,\n",
    "                             depth_multiplier=d,\n",
    "                             depthwise_constraint=max_norm(1.))(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = Activation('elu')(block1)\n",
    "    block1 = AveragePooling2D((1, 4))(block1)\n",
    "    block1 = dropout_type(dropout_rate)(block1)\n",
    "\n",
    "    block2 = SeparableConv2D(f2, (1, 16),\n",
    "                             use_bias=False, padding='same')(block1)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "    block2 = AveragePooling2D((1, 8))(block2)\n",
    "    block2 = dropout_type(dropout_rate)(block2)\n",
    "\n",
    "    flatten = Flatten(name='flatten')(block2)\n",
    "\n",
    "    dense = Dense(nb_classes, name='dense',\n",
    "                  kernel_constraint=max_norm(norm_rate))(flatten)\n",
    "    softmax = Activation('softmax', name='softmax')(dense)\n",
    "\n",
    "    return Model(inputs=input1, outputs=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train conv model\n",
    "eeg_net = EEGNet(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_net.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/6\n",
      "303/303 [==============================] - 61s 198ms/step - loss: 0.4790 - sparse_categorical_accuracy: 0.8165 - val_loss: 0.5260 - val_sparse_categorical_accuracy: 0.8229\n",
      "Epoch 2/6\n",
      "303/303 [==============================] - 49s 162ms/step - loss: 0.4225 - sparse_categorical_accuracy: 0.8207 - val_loss: 0.4957 - val_sparse_categorical_accuracy: 0.8003\n",
      "Epoch 3/6\n",
      "303/303 [==============================] - 47s 156ms/step - loss: 0.4135 - sparse_categorical_accuracy: 0.8215 - val_loss: 0.6550 - val_sparse_categorical_accuracy: 0.7628\n",
      "Epoch 4/6\n",
      "303/303 [==============================] - 48s 158ms/step - loss: 0.4058 - sparse_categorical_accuracy: 0.8202 - val_loss: 1.0603 - val_sparse_categorical_accuracy: 0.3896\n",
      "Epoch 5/6\n",
      "303/303 [==============================] - 47s 156ms/step - loss: 0.4000 - sparse_categorical_accuracy: 0.8186 - val_loss: 1.2473 - val_sparse_categorical_accuracy: 0.7987\n",
      "Epoch 6/6\n",
      "303/303 [==============================] - 52s 173ms/step - loss: 0.3971 - sparse_categorical_accuracy: 0.8214 - val_loss: 0.5968 - val_sparse_categorical_accuracy: 0.6696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4790300130844116,\n",
       "  0.42250344157218933,\n",
       "  0.41348060965538025,\n",
       "  0.4058060050010681,\n",
       "  0.40003502368927,\n",
       "  0.3971465826034546],\n",
       " 'sparse_categorical_accuracy': [0.8164700865745544,\n",
       "  0.8207117915153503,\n",
       "  0.8215394020080566,\n",
       "  0.8201944828033447,\n",
       "  0.8186426758766174,\n",
       "  0.8214359879493713],\n",
       " 'val_loss': [0.5260092616081238,\n",
       "  0.49569565057754517,\n",
       "  0.6549774408340454,\n",
       "  1.0602548122406006,\n",
       "  1.2473222017288208,\n",
       "  0.59682697057724],\n",
       " 'val_sparse_categorical_accuracy': [0.8228673934936523,\n",
       "  0.8002697825431824,\n",
       "  0.7627913355827332,\n",
       "  0.389643132686615,\n",
       "  0.798721969127655,\n",
       "  0.6695927381515503]}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Fit model on training data')\n",
    "history = eeg_net.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=6,\n",
    "    validation_data=(x_dev, y_dev),\n",
    ")\n",
    "history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "354/354 [==============================] - 25s 70ms/step - loss: 0.5968 - sparse_categorical_accuracy: 0.6696\n",
      "test loss, test acc: [0.5968274474143982, 0.6695927381515503]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = eeg_net.evaluate(x_dev, y_dev, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try models from article sent by raj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator FunctionTransformer from version 0.20.3 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator FeatureUnion from version 0.20.3 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator ExtraTreeClassifier from version 0.20.3 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator ExtraTreesClassifier from version 0.20.3 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 0.20.3 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.preprocessing.data module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.preprocessing. Anything that cannot be imported from sklearn.preprocessing is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 0.20.3 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the model, columns, and scaler\n",
    "best_model = pickle.load(open('best_classifier.pkl','rb'))\n",
    "cols_input = pickle.load(open('cols_input.sav','rb'))\n",
    "scaler = pickle.load(open('scaler.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The reset parameter is False but there is no n_features_in_ attribute. Is this estimator fitted?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-e5e3901b56c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# transform our data matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX_train_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mX_valid_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mX_test_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         X = self._validate_data(X, reset=False,\n\u001b[0m\u001b[1;32m    792\u001b[0m                                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensure_2d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_features_in_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    373\u001b[0m                     \u001b[0;34m\"The reset parameter is False but there is no \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                     \u001b[0;34m\"n_features_in_ attribute. Is this estimator fitted?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The reset parameter is False but there is no n_features_in_ attribute. Is this estimator fitted?"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_valid= pd.read_csv('df_valid.csv')\n",
    "df_test= pd.read_csv('df_test.csv')\n",
    "\n",
    "# create X and y matrices\n",
    "X_train = df_train[cols_input].values\n",
    "X_valid = df_valid[cols_input].values\n",
    "X_test = df_test[cols_input].values\n",
    "\n",
    "y_train = df_train['OUTPUT_LABEL'].values\n",
    "y_valid = df_valid['OUTPUT_LABEL'].values\n",
    "y_test = df_test['OUTPUT_LABEL'].values\n",
    "\n",
    "# transform our data matrices \n",
    "X_train_tf = scaler.transform(X_train)\n",
    "X_valid_tf = scaler.transform(X_valid)\n",
    "X_test_tf = scaler.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
