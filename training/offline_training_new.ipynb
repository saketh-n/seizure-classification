{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"epilepsy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X21.V1.791</td>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>...</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X15.V1.924</td>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>...</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X8.V1.1</td>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X16.V1.60</td>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>...</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X20.V1.54</td>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 180 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   X1   X2   X3   X4   X5   X6   X7   X8   X9  ...  X170  X171  \\\n",
       "0  X21.V1.791  135  190  229  223  192  125   55   -9  -33  ...   -17   -15   \n",
       "1  X15.V1.924  386  382  356  331  320  315  307  272  244  ...   164   150   \n",
       "2     X8.V1.1  -32  -39  -47  -37  -32  -36  -57  -73  -85  ...    57    64   \n",
       "3   X16.V1.60 -105 -101  -96  -92  -89  -95 -102 -100  -87  ...   -82   -81   \n",
       "4   X20.V1.54   -9  -65  -98 -102  -78  -48  -16    0  -21  ...     4     2   \n",
       "\n",
       "   X172  X173  X174  X175  X176  X177  X178  y  \n",
       "0   -31   -77  -103  -127  -116   -83   -51  4  \n",
       "1   146   152   157   156   154   143   129  1  \n",
       "2    48    19   -12   -30   -35   -35   -36  5  \n",
       "3   -80   -77   -85   -77   -72   -69   -65  5  \n",
       "4   -12   -32   -41   -65   -83   -89   -73  5  \n",
       "\n",
       "[5 rows x 180 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"OUTPUT_LABEL\"] = df.y == 1\n",
    "df[\"OUTPUT_LABEL\"] = df[\"OUTPUT_LABEL\"].astype(int)\n",
    "df.pop('y')\n",
    "df.drop(df.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X170</th>\n",
       "      <th>X171</th>\n",
       "      <th>X172</th>\n",
       "      <th>X173</th>\n",
       "      <th>X174</th>\n",
       "      <th>X175</th>\n",
       "      <th>X176</th>\n",
       "      <th>X177</th>\n",
       "      <th>X178</th>\n",
       "      <th>OUTPUT_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135</td>\n",
       "      <td>190</td>\n",
       "      <td>229</td>\n",
       "      <td>223</td>\n",
       "      <td>192</td>\n",
       "      <td>125</td>\n",
       "      <td>55</td>\n",
       "      <td>-9</td>\n",
       "      <td>-33</td>\n",
       "      <td>-38</td>\n",
       "      <td>...</td>\n",
       "      <td>-17</td>\n",
       "      <td>-15</td>\n",
       "      <td>-31</td>\n",
       "      <td>-77</td>\n",
       "      <td>-103</td>\n",
       "      <td>-127</td>\n",
       "      <td>-116</td>\n",
       "      <td>-83</td>\n",
       "      <td>-51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>382</td>\n",
       "      <td>356</td>\n",
       "      <td>331</td>\n",
       "      <td>320</td>\n",
       "      <td>315</td>\n",
       "      <td>307</td>\n",
       "      <td>272</td>\n",
       "      <td>244</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>164</td>\n",
       "      <td>150</td>\n",
       "      <td>146</td>\n",
       "      <td>152</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>154</td>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-32</td>\n",
       "      <td>-39</td>\n",
       "      <td>-47</td>\n",
       "      <td>-37</td>\n",
       "      <td>-32</td>\n",
       "      <td>-36</td>\n",
       "      <td>-57</td>\n",
       "      <td>-73</td>\n",
       "      <td>-85</td>\n",
       "      <td>-94</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>64</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>-12</td>\n",
       "      <td>-30</td>\n",
       "      <td>-35</td>\n",
       "      <td>-35</td>\n",
       "      <td>-36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-105</td>\n",
       "      <td>-101</td>\n",
       "      <td>-96</td>\n",
       "      <td>-92</td>\n",
       "      <td>-89</td>\n",
       "      <td>-95</td>\n",
       "      <td>-102</td>\n",
       "      <td>-100</td>\n",
       "      <td>-87</td>\n",
       "      <td>-79</td>\n",
       "      <td>...</td>\n",
       "      <td>-82</td>\n",
       "      <td>-81</td>\n",
       "      <td>-80</td>\n",
       "      <td>-77</td>\n",
       "      <td>-85</td>\n",
       "      <td>-77</td>\n",
       "      <td>-72</td>\n",
       "      <td>-69</td>\n",
       "      <td>-65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9</td>\n",
       "      <td>-65</td>\n",
       "      <td>-98</td>\n",
       "      <td>-102</td>\n",
       "      <td>-78</td>\n",
       "      <td>-48</td>\n",
       "      <td>-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-21</td>\n",
       "      <td>-59</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-12</td>\n",
       "      <td>-32</td>\n",
       "      <td>-41</td>\n",
       "      <td>-65</td>\n",
       "      <td>-83</td>\n",
       "      <td>-89</td>\n",
       "      <td>-73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  ...  X170  X171  X172  \\\n",
       "0  135  190  229  223  192  125   55   -9  -33  -38  ...   -17   -15   -31   \n",
       "1  386  382  356  331  320  315  307  272  244  232  ...   164   150   146   \n",
       "2  -32  -39  -47  -37  -32  -36  -57  -73  -85  -94  ...    57    64    48   \n",
       "3 -105 -101  -96  -92  -89  -95 -102 -100  -87  -79  ...   -82   -81   -80   \n",
       "4   -9  -65  -98 -102  -78  -48  -16    0  -21  -59  ...     4     2   -12   \n",
       "\n",
       "   X173  X174  X175  X176  X177  X178  OUTPUT_LABEL  \n",
       "0   -77  -103  -127  -116   -83   -51             0  \n",
       "1   152   157   156   154   143   129             1  \n",
       "2    19   -12   -30   -35   -35   -36             0  \n",
       "3   -77   -85   -77   -72   -69   -65             0  \n",
       "4   -32   -41   -65   -83   -89   -73             0  \n",
       "\n",
       "[5 rows x 179 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prevalence of the positive class: 0.200\n"
     ]
    }
   ],
   "source": [
    "def calc_prevalence(y_actual):\n",
    "    # this function calculates the prevalence of the positive class (label = 1)\n",
    "    return sum(y_actual) / len(y_actual)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"prevalence of the positive class: %.3f\"\n",
    "    % calc_prevalence(df[\"OUTPUT_LABEL\"].values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Columns: 179\n"
     ]
    }
   ],
   "source": [
    "print(\"# of Columns:\", len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist = df.columns.tolist()\n",
    "cols_input = collist[0:178]\n",
    "df_data = df[cols_input + [\"OUTPUT_LABEL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# check for duplicated columns in cols_input\n",
    "dup_cols = set([x for x in cols_input if cols_input.count(x) > 1])\n",
    "print(dup_cols)\n",
    "assert len(dup_cols) == 0, \"you have duplicated columns in cols_input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# check for duplicated columns in df_data\n",
    "cols_df_data = list(df_data.columns)\n",
    "dup_cols = set([x for x in cols_df_data if cols_df_data.count(x) > 1])\n",
    "print(dup_cols)\n",
    "assert len(dup_cols) == 0,'you have duplicated columns in df_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the size of df_data makes sense\n",
    "assert (len(cols_input) + 1) == len(\n",
    "    df_data.columns\n",
    "), \"issue with dimensions of df_data or cols_input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.sample(n=len(df_data))\n",
    "df_data = df_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Split Size: 0.3\n"
     ]
    }
   ],
   "source": [
    "df_valid_test = df_data.sample(frac=0.3)\n",
    "print(\"Validation/Test Split Size: %.1f\" % (len(df_valid_test) / len(df_data)))\n",
    "\n",
    "df_test = df_valid_test.sample(frac=0.5)\n",
    "df_valid = df_valid_test.drop(df_test.index)\n",
    "\n",
    "df_train_all = df_data.drop(df_valid_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prevalence(n = 1725):0.209\n",
      "Valid prevalence(n = 1725):0.183\n",
      "Train all prevalence(n = 8050):0.202\n"
     ]
    }
   ],
   "source": [
    "# check the prevalence of each\n",
    "print(\n",
    "    \"Test prevalence(n = %d):%.3f\"\n",
    "    % (len(df_test), calc_prevalence(df_test.OUTPUT_LABEL.values))\n",
    ")\n",
    "print(\n",
    "    \"Valid prevalence(n = %d):%.3f\"\n",
    "    % (len(df_valid), calc_prevalence(df_valid.OUTPUT_LABEL.values))\n",
    ")\n",
    "print(\n",
    "    \"Train all prevalence(n = %d):%.3f\"\n",
    "    % (len(df_train_all), calc_prevalence(df_train_all.OUTPUT_LABEL.values))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples (n = 11500)\n"
     ]
    }
   ],
   "source": [
    "print('all samples (n = %d)'%len(df_data))\n",
    "assert len(df_data) == (len(df_test)+len(df_valid)+len(df_train_all)),'math didnt work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train balanced prevalence(n = 3248):0.500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rows_pos = df_train_all.OUTPUT_LABEL == 1\n",
    "df_train_pos = df_train_all.loc[rows_pos]\n",
    "df_train_neg = df_train_all.loc[~rows_pos]\n",
    "\n",
    "n = np.min([len(df_train_pos), len(df_train_neg)])\n",
    "\n",
    "df_train = pd.concat([df_train_pos.sample(n=n, random_state=69), df_train_neg.sample(n=n, random_state=69)], axis=0, ignore_index=True)\n",
    "\n",
    "df_train = df_train.sample(n=len(df_train), random_state=69).reset_index(drop=True)\n",
    "\n",
    "print('Train balanced prevalence(n = %d):%.3f'%(len(df_train), calc_prevalence(df_train.OUTPUT_LABEL.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.to_csv('df_train_all.csv',index=False)\n",
    "df_train.to_csv('df_train.csv',index=False)\n",
    "df_valid.to_csv('df_valid.csv',index=False)\n",
    "df_test.to_csv('df_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(cols_input, open('cols_input.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to fill missing values with mean of the column if needed\n",
    "def fill_my_missing(df, df_mean, col2use):\n",
    "    # This function fills the missing values\n",
    "\n",
    "    # check the columns are present\n",
    "    for c in col2use:\n",
    "        assert c in df.columns, c + ' not in df'\n",
    "        assert c in df_mean.col.values, c+ 'not in df_mean'\n",
    "    \n",
    "    # replace the mean \n",
    "    for c in col2use:\n",
    "        mean_value = df_mean.loc[df_mean.col == c,'mean_val'].values[0]\n",
    "        df[c] = df[c].fillna(mean_value)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training All shapes: (8050, 178)\n",
      "Training shapes: (3248, 178) (3248,)\n",
      "Validation shapes: (1725, 178) (1725,)\n"
     ]
    }
   ],
   "source": [
    "# create the X and y matrices\n",
    "X_train = df_train[cols_input].values\n",
    "X_train_all = df_train_all[cols_input].values\n",
    "X_valid = df_valid[cols_input].values\n",
    "\n",
    "y_train = df_train['OUTPUT_LABEL'].values\n",
    "y_valid = df_valid['OUTPUT_LABEL'].values\n",
    "\n",
    "print('Training All shapes:',X_train_all.shape)\n",
    "print('Training shapes:',X_train.shape, y_train.shape)\n",
    "print('Validation shapes:',X_valid.shape, y_valid.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler  = StandardScaler()\n",
    "scaler.fit(X_train_all)\n",
    "\n",
    "scalerfile = 'scaler.sav'\n",
    "pickle.dump(scaler, open(scalerfile, 'wb'))\n",
    "scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "\n",
    "# transform our data matrices\n",
    "X_train_tf = scaler.transform(X_train)\n",
    "X_valid_tf = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "\n",
    "def print_report(y_actual, y_pred, thresh):\n",
    "    \n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print('prevalence:%.3f'%calc_prevalence(y_actual))\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Training:\n",
      "AUC:0.992\n",
      "accuracy:0.628\n",
      "recall:0.257\n",
      "precision:1.000\n",
      "specificity:1.000\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.964\n",
      "accuracy:0.860\n",
      "recall:0.235\n",
      "precision:1.000\n",
      "specificity:1.000\n",
      "prevalence:0.183\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors = 100)\n",
    "knn.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = knn.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = knn.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('KNN')\n",
    "print('Training:')\n",
    "knn_train_auc, knn_train_accuracy, knn_train_recall, \\\n",
    "    knn_train_precision, knn_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "knn_valid_auc, knn_valid_accuracy, knn_valid_recall, \\\n",
    "    knn_valid_precision, knn_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Training:\n",
      "AUC:0.615\n",
      "accuracy:0.641\n",
      "recall:0.529\n",
      "precision:0.682\n",
      "specificity:0.753\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.483\n",
      "accuracy:0.670\n",
      "recall:0.419\n",
      "precision:0.254\n",
      "specificity:0.726\n",
      "prevalence:0.183\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state = 69)\n",
    "lr.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = lr.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = lr.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Logistic Regression')\n",
    "print('Training:')\n",
    "lr_train_auc, lr_train_accuracy, lr_train_recall, \\\n",
    "    lr_train_precision, lr_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
    "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDC\n",
      "Training:\n",
      "AUC:0.569\n",
      "accuracy:0.590\n",
      "recall:0.517\n",
      "precision:0.605\n",
      "specificity:0.662\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.464\n",
      "accuracy:0.597\n",
      "recall:0.432\n",
      "precision:0.209\n",
      "specificity:0.634\n",
      "prevalence:0.183\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "SGDC = SGDClassifier(loss = 'log',alpha = 0.1)\n",
    "SGDC.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = SGDC.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = SGDC.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('SGDC')\n",
    "print('Training:')\n",
    "SGDC_train_auc, SGDC_train_accuracy, SGDC_train_recall, \\\n",
    "    SGDC_train_precision, SGDC_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "SGDC_valid_auc, SGDC_valid_accuracy, SGDC_valid_recall, \\\n",
    "    SGDC_valid_precision, SGDC_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Training:\n",
      "AUC:0.983\n",
      "accuracy:0.935\n",
      "recall:0.897\n",
      "precision:0.971\n",
      "specificity:0.974\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.985\n",
      "accuracy:0.961\n",
      "recall:0.892\n",
      "precision:0.892\n",
      "specificity:0.976\n",
      "prevalence:0.183\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = nb.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = nb.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Naive Bayes')\n",
    "print('Training:')\n",
    "nb_train_auc, nb_train_accuracy, nb_train_recall, nb_train_precision, \\\n",
    "nb_train_specificity =print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "nb_valid_auc, nb_valid_accuracy, nb_valid_recall, nb_valid_precision, \\\n",
    "nb_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+000, 1.00000000e+000, 1.00000000e+000, ...,\n",
       "       1.92930137e-097, 2.94613393e-035, 8.27595627e-120])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.03426349e-080, 5.64475218e-111, 9.32442717e-109, ...,\n",
       "       3.27341106e-098, 1.00000000e+000, 5.52677700e-082])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nb, open('nb_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3248, 178)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redo of data processing for TUSZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tusz_subset\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_dev, y_dev) = tfds.as_numpy(tfds.load('tusz_subset',\n",
    "#                                                              split=['train', 'dev'],\n",
    "#                                                              batch_size=-1,\n",
    "#                                                              as_supervised=True))\n",
    "(x_dev, y_dev), (x_train, y_train) = tfds.as_numpy(tfds.load('tusz_subset',\n",
    "                                                             split=['train', 'dev'],\n",
    "                                                             batch_size=-1,\n",
    "                                                             as_supervised=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dev.squeeze(-1).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev_compressed = x_dev.squeeze(-1)\n",
    "num_samples = x_dev_compressed.shape[0]\n",
    "num_channels = x_dev_compressed.shape[1]\n",
    "num_data_points = x_dev_compressed.shape[2]\n",
    "x_dev_combo = np.zeros((num_samples, num_channels * num_data_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(num_samples):\n",
    "    combo_rows = np.concatenate([x for x in x_dev_compressed[idx]], axis=None)\n",
    "    x_dev_combo[idx] = combo_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev_df = pd.DataFrame(x_dev_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev_df.loc[:, 'OUTPUT_LABEL'] = pd.DataFrame(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>8183</th>\n",
       "      <th>8184</th>\n",
       "      <th>8185</th>\n",
       "      <th>8186</th>\n",
       "      <th>8187</th>\n",
       "      <th>8188</th>\n",
       "      <th>8189</th>\n",
       "      <th>8190</th>\n",
       "      <th>8191</th>\n",
       "      <th>OUTPUT_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>-2.009131e-06</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>7.407766e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>5.250351e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.144214e-06</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>1.467795e-05</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-1.325918e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-2.703569e-05</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-2.130927e-05</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-9.893480e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-1.275558e-05</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-5.830710e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>4.513475e-07</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-2.511009e-05</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-2.382859e-05</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-1.781836e-05</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9661</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.342690e-05</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>2.786120e-07</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-2.242856e-05</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9662</th>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-1.896149e-05</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-1.852566e-05</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-1.671547e-05</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9663</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-3.041991e-07</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-1.879407e-05</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-1.631442e-05</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664</th>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-3.453168e-05</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-4.038986e-05</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-3.665712e-05</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9665</th>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-3.620658e-05</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-3.343782e-05</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-2.170386e-05</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9666 rows × 8193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1             2         3         4             5  \\\n",
       "0     0.000008  0.000004 -2.009131e-06 -0.000006 -0.000005  7.407766e-06   \n",
       "1     0.000005  0.000001  3.144214e-06  0.000012  0.000018  1.467795e-05   \n",
       "2    -0.000028 -0.000028 -2.703569e-05 -0.000027 -0.000024 -2.130927e-05   \n",
       "3    -0.000013 -0.000013 -1.275558e-05 -0.000011 -0.000008 -5.830710e-06   \n",
       "4    -0.000027 -0.000024 -2.511009e-05 -0.000037 -0.000030 -2.382859e-05   \n",
       "...        ...       ...           ...       ...       ...           ...   \n",
       "9661  0.000002  0.000007  1.342690e-05  0.000014  0.000005  2.786120e-07   \n",
       "9662 -0.000019 -0.000019 -1.896149e-05 -0.000019 -0.000019 -1.852566e-05   \n",
       "9663  0.000010  0.000005 -3.041991e-07 -0.000007 -0.000015 -1.879407e-05   \n",
       "9664 -0.000016 -0.000025 -3.453168e-05 -0.000042 -0.000042 -4.038986e-05   \n",
       "9665 -0.000050 -0.000049 -3.620658e-05 -0.000028 -0.000027 -3.343782e-05   \n",
       "\n",
       "             6         7             8         9  ...  8183  8184  8185  8186  \\\n",
       "0     0.000017  0.000017  5.250351e-06 -0.000003  ...   0.0   0.0   0.0   0.0   \n",
       "1     0.000006 -0.000001 -1.325918e-06  0.000003  ...   0.0   0.0   0.0   0.0   \n",
       "2    -0.000016 -0.000011 -9.893480e-06 -0.000010  ...   0.0   0.0   0.0   0.0   \n",
       "3    -0.000004 -0.000001  4.513475e-07  0.000003  ...   0.0   0.0   0.0   0.0   \n",
       "4    -0.000015 -0.000012 -1.781836e-05 -0.000029  ...   0.0   0.0   0.0   0.0   \n",
       "...        ...       ...           ...       ...  ...   ...   ...   ...   ...   \n",
       "9661 -0.000012 -0.000023 -2.242856e-05 -0.000022  ...   0.0   0.0   0.0   0.0   \n",
       "9662 -0.000017 -0.000016 -1.671547e-05 -0.000018  ...   0.0   0.0   0.0   0.0   \n",
       "9663 -0.000020 -0.000019 -1.631442e-05 -0.000015  ...   0.0   0.0   0.0   0.0   \n",
       "9664 -0.000038 -0.000037 -3.665712e-05 -0.000038  ...   0.0   0.0   0.0   0.0   \n",
       "9665 -0.000034 -0.000027 -2.170386e-05 -0.000018  ...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "      8187  8188  8189  8190  8191  OUTPUT_LABEL  \n",
       "0      0.0   0.0   0.0   0.0   0.0             0  \n",
       "1      0.0   0.0   0.0   0.0   0.0             0  \n",
       "2      0.0   0.0   0.0   0.0   0.0             0  \n",
       "3      0.0   0.0   0.0   0.0   0.0             0  \n",
       "4      0.0   0.0   0.0   0.0   0.0             1  \n",
       "...    ...   ...   ...   ...   ...           ...  \n",
       "9661   0.0   0.0   0.0   0.0   0.0             1  \n",
       "9662   0.0   0.0   0.0   0.0   0.0             0  \n",
       "9663   0.0   0.0   0.0   0.0   0.0             0  \n",
       "9664   0.0   0.0   0.0   0.0   0.0             1  \n",
       "9665   0.0   0.0   0.0   0.0   0.0             0  \n",
       "\n",
       "[9666 rows x 8193 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_compressed = x_train.squeeze(-1)\n",
    "num_samples = x_train_compressed.shape[0]\n",
    "num_channels = x_train_compressed.shape[1]\n",
    "num_data_points = x_train_compressed.shape[2]\n",
    "x_train_combo = np.zeros((num_samples, num_channels * num_data_points))\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    combo_rows = np.concatenate([x for x in x_train_compressed[idx]], axis=None)\n",
    "    x_train_combo[idx] = combo_rows\n",
    "\n",
    "x_train_df = pd.DataFrame(x_train_combo)\n",
    "x_train_df.loc[:, 'OUTPUT_LABEL'] = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prevalence of the positive class in training set: 0.175\n",
      "prevalence of the positive class in dev set: 0.177\n"
     ]
    }
   ],
   "source": [
    "def calc_prevalence(y_actual):\n",
    "    # this function calculates the prevalence of the positive class (label = 1)\n",
    "    return sum(y_actual) / len(y_actual)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"prevalence of the positive class in training set: %.3f\"\n",
    "    % calc_prevalence(x_train_df[\"OUTPUT_LABEL\"].values)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"prevalence of the positive class in dev set: %.3f\"\n",
    "    % calc_prevalence(x_dev_df[\"OUTPUT_LABEL\"].values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = x_train_df.append(x_dev_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>8183</th>\n",
       "      <th>8184</th>\n",
       "      <th>8185</th>\n",
       "      <th>8186</th>\n",
       "      <th>8187</th>\n",
       "      <th>8188</th>\n",
       "      <th>8189</th>\n",
       "      <th>8190</th>\n",
       "      <th>8191</th>\n",
       "      <th>OUTPUT_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>8.992812e-05</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>8.287322e-05</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>...</td>\n",
       "      <td>9.990785e-05</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>9.246213e-05</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>9.114950e-05</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>9.012983e-05</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>8.012478e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-3.123867e-05</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-3.077695e-05</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>1.747468e-07</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-6.354389e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-2.821123e-06</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-9.866826e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>2.071281e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-6.596678e-06</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-1.094951e-05</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.729772e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>8.841563e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>9.944937e-07</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-4.438979e-06</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-1.235643e-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-2.561036e-05</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-3.393891e-05</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.484936e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-1.245930e-05</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-1.089710e-05</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-5.458865e-06</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-5.981176e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>8.979600e-06</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-1.665847e-05</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.521782e-06</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-1.635409e-05</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-3.646994e-05</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-5.797111e-05</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-2.152212e-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54887</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.342690e-05</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>2.786120e-07</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54888</th>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-1.896149e-05</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-1.852566e-05</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54889</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-3.041991e-07</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-1.879407e-05</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54890</th>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-3.453168e-05</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-4.038986e-05</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54891</th>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-3.620658e-05</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-3.343782e-05</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54892 rows × 8193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1             2         3         4             5  \\\n",
       "0      0.000092  0.000088  8.992812e-05  0.000091  0.000089  8.287322e-05   \n",
       "1     -0.000029 -0.000029 -3.123867e-05 -0.000031 -0.000030 -3.077695e-05   \n",
       "2      0.000002 -0.000002 -6.596678e-06 -0.000006 -0.000010 -1.094951e-05   \n",
       "3     -0.000014 -0.000021 -2.561036e-05 -0.000028 -0.000032 -3.393891e-05   \n",
       "4      0.000013  0.000015  8.979600e-06 -0.000001 -0.000009 -1.665847e-05   \n",
       "...         ...       ...           ...       ...       ...           ...   \n",
       "54887  0.000002  0.000007  1.342690e-05  0.000014  0.000005  2.786120e-07   \n",
       "54888 -0.000019 -0.000019 -1.896149e-05 -0.000019 -0.000019 -1.852566e-05   \n",
       "54889  0.000010  0.000005 -3.041991e-07 -0.000007 -0.000015 -1.879407e-05   \n",
       "54890 -0.000016 -0.000025 -3.453168e-05 -0.000042 -0.000042 -4.038986e-05   \n",
       "54891 -0.000050 -0.000049 -3.620658e-05 -0.000028 -0.000027 -3.343782e-05   \n",
       "\n",
       "              6         7         8         9  ...          8183      8184  \\\n",
       "0      0.000096  0.000082  0.000083  0.000081  ...  9.990785e-05  0.000086   \n",
       "1     -0.000031 -0.000031 -0.000031 -0.000030  ...  1.747468e-07 -0.000005   \n",
       "2     -0.000010 -0.000012 -0.000010 -0.000011  ... -1.729772e-06  0.000001   \n",
       "3     -0.000031 -0.000019 -0.000033 -0.000033  ... -7.484936e-06 -0.000008   \n",
       "4     -0.000017 -0.000017 -0.000019 -0.000020  ... -6.521782e-06 -0.000009   \n",
       "...         ...       ...       ...       ...  ...           ...       ...   \n",
       "54887 -0.000012 -0.000023 -0.000022 -0.000022  ...  0.000000e+00  0.000000   \n",
       "54888 -0.000017 -0.000016 -0.000017 -0.000018  ...  0.000000e+00  0.000000   \n",
       "54889 -0.000020 -0.000019 -0.000016 -0.000015  ...  0.000000e+00  0.000000   \n",
       "54890 -0.000038 -0.000037 -0.000037 -0.000038  ...  0.000000e+00  0.000000   \n",
       "54891 -0.000034 -0.000027 -0.000022 -0.000018  ...  0.000000e+00  0.000000   \n",
       "\n",
       "               8185      8186          8187      8188          8189      8190  \\\n",
       "0      9.246213e-05  0.000104  9.114950e-05  0.000116  9.012983e-05  0.000083   \n",
       "1     -6.354389e-06 -0.000003 -2.821123e-06 -0.000005 -9.866826e-07 -0.000002   \n",
       "2      8.841563e-07  0.000002  9.944937e-07 -0.000003 -4.438979e-06 -0.000007   \n",
       "3     -1.245930e-05 -0.000007 -1.089710e-05 -0.000012 -5.458865e-06 -0.000006   \n",
       "4     -1.635409e-05 -0.000029 -3.646994e-05 -0.000048 -5.797111e-05 -0.000049   \n",
       "...             ...       ...           ...       ...           ...       ...   \n",
       "54887  0.000000e+00  0.000000  0.000000e+00  0.000000  0.000000e+00  0.000000   \n",
       "54888  0.000000e+00  0.000000  0.000000e+00  0.000000  0.000000e+00  0.000000   \n",
       "54889  0.000000e+00  0.000000  0.000000e+00  0.000000  0.000000e+00  0.000000   \n",
       "54890  0.000000e+00  0.000000  0.000000e+00  0.000000  0.000000e+00  0.000000   \n",
       "54891  0.000000e+00  0.000000  0.000000e+00  0.000000  0.000000e+00  0.000000   \n",
       "\n",
       "               8191  OUTPUT_LABEL  \n",
       "0      8.012478e-05             0  \n",
       "1      2.071281e-07             0  \n",
       "2     -1.235643e-06             1  \n",
       "3     -5.981176e-06             0  \n",
       "4     -2.152212e-05             1  \n",
       "...             ...           ...  \n",
       "54887  0.000000e+00             1  \n",
       "54888  0.000000e+00             0  \n",
       "54889  0.000000e+00             0  \n",
       "54890  0.000000e+00             1  \n",
       "54891  0.000000e+00             0  \n",
       "\n",
       "[54892 rows x 8193 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist = combined_data.columns.tolist()\n",
    "cols_input = collist[0:8192]\n",
    "df_data = combined_data[cols_input + [\"OUTPUT_LABEL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# check for duplicated columns in cols_input\n",
    "dup_cols = set([x for x in cols_input if cols_input.count(x) > 1])\n",
    "print(dup_cols)\n",
    "assert len(dup_cols) == 0, \"you have duplicated columns in cols_input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# check for duplicated columns in df_data\n",
    "cols_df_data = list(df_data.columns)\n",
    "dup_cols = set([x for x in cols_df_data if cols_df_data.count(x) > 1])\n",
    "print(dup_cols)\n",
    "assert len(dup_cols) == 0,'you have duplicated columns in df_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the size of df_data makes sense\n",
    "assert (len(cols_input) + 1) == len(\n",
    "    df_data.columns\n",
    "), \"issue with dimensions of df_data or cols_input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.sample(n=len(df_data))\n",
    "df_data = df_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Split Size: 0.3\n"
     ]
    }
   ],
   "source": [
    "df_valid_test = df_data.sample(frac=0.3)\n",
    "print(\"Validation/Test Split Size: %.1f\" % (len(df_valid_test) / len(df_data)))\n",
    "\n",
    "df_test = df_valid_test.sample(frac=0.5)\n",
    "df_valid = df_valid_test.drop(df_test.index)\n",
    "\n",
    "df_train_all = df_data.drop(df_valid_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prevalence(n = 8234):0.174\n",
      "Valid prevalence(n = 8234):0.179\n",
      "Train all prevalence(n = 38424):0.175\n"
     ]
    }
   ],
   "source": [
    "# check the prevalence of each\n",
    "print(\n",
    "    \"Test prevalence(n = %d):%.3f\"\n",
    "    % (len(df_test), calc_prevalence(df_test.OUTPUT_LABEL.values))\n",
    ")\n",
    "print(\n",
    "    \"Valid prevalence(n = %d):%.3f\"\n",
    "    % (len(df_valid), calc_prevalence(df_valid.OUTPUT_LABEL.values))\n",
    ")\n",
    "print(\n",
    "    \"Train all prevalence(n = %d):%.3f\"\n",
    "    % (len(df_train_all), calc_prevalence(df_train_all.OUTPUT_LABEL.values))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples (n = 54892)\n"
     ]
    }
   ],
   "source": [
    "print('all samples (n = %d)'%len(df_data))\n",
    "assert len(df_data) == (len(df_test)+len(df_valid)+len(df_train_all)),'math didnt work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train balanced prevalence(n = 13436):0.500\n"
     ]
    }
   ],
   "source": [
    "rows_pos = df_train_all.OUTPUT_LABEL == 1\n",
    "df_train_pos = df_train_all.loc[rows_pos]\n",
    "df_train_neg = df_train_all.loc[~rows_pos]\n",
    "\n",
    "n = np.min([len(df_train_pos), len(df_train_neg)])\n",
    "\n",
    "df_train = pd.concat([df_train_pos.sample(n=n, random_state=69), df_train_neg.sample(n=n, random_state=69)], axis=0, ignore_index=True)\n",
    "\n",
    "df_train = df_train.sample(n=len(df_train), random_state=69).reset_index(drop=True)\n",
    "\n",
    "print('Train balanced prevalence(n = %d):%.3f'%(len(df_train), calc_prevalence(df_train.OUTPUT_LABEL.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.to_csv('df_train_all_tusz.csv',index=False)\n",
    "df_train.to_csv('df_train_tusz.csv',index=False)\n",
    "df_valid.to_csv('df_valid_tusz.csv',index=False)\n",
    "df_test.to_csv('df_test_tusz.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(cols_input, open('cols_input_tusz.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training All shapes: (38424, 8192)\n",
      "Training shapes: (13436, 8192) (13436,)\n",
      "Validation shapes: (8234, 8192) (8234,)\n"
     ]
    }
   ],
   "source": [
    "# create the X and y matrices\n",
    "X_train = df_train[cols_input].values\n",
    "X_train_all = df_train_all[cols_input].values\n",
    "X_valid = df_valid[cols_input].values\n",
    "\n",
    "y_train = df_train['OUTPUT_LABEL'].values\n",
    "y_valid = df_valid['OUTPUT_LABEL'].values\n",
    "\n",
    "print('Training All shapes:',X_train_all.shape)\n",
    "print('Training shapes:',X_train.shape, y_train.shape)\n",
    "print('Validation shapes:',X_valid.shape, y_valid.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler  = StandardScaler()\n",
    "scaler.fit(X_train_all)\n",
    "\n",
    "scalerfile = 'scaler_tusz.sav'\n",
    "pickle.dump(scaler, open(scalerfile, 'wb'))\n",
    "scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "\n",
    "# transform our data matrices\n",
    "X_train_tf = scaler.transform(X_train)\n",
    "X_valid_tf = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "\n",
    "def print_report(y_actual, y_pred, thresh):\n",
    "    \n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print('prevalence:%.3f'%calc_prevalence(y_actual))\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors = 100)\n",
    "knn.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = knn.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = knn.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('KNN')\n",
    "print('Training:')\n",
    "knn_train_auc, knn_train_accuracy, knn_train_recall, \\\n",
    "    knn_train_precision, knn_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "knn_valid_auc, knn_valid_accuracy, knn_valid_recall, \\\n",
    "    knn_valid_precision, knn_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to train on average of all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tusz_subset\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_dev, y_dev), (x_train, y_train) = tfds.as_numpy(tfds.load('tusz_subset',\n",
    "                                                             split=['train', 'dev'],\n",
    "                                                             batch_size=-1,\n",
    "                                                             as_supervised=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev_compressed = x_dev.squeeze(-1)\n",
    "num_samples = x_dev_compressed.shape[0]\n",
    "num_channels = x_dev_compressed.shape[1]\n",
    "num_data_points = x_dev_compressed.shape[2]\n",
    "x_dev_combo = np.zeros((num_samples, num_data_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(num_samples):\n",
    "    x_dev_combo[idx] = x_dev_compressed[idx].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev_df = pd.DataFrame(x_dev_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev_df.loc[:, 'OUTPUT_LABEL'] = pd.DataFrame(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>OUTPUT_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-4.339751e-06</td>\n",
       "      <td>-9.557706e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-2.562251e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-2.090641e-05</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-7.125459e-07</td>\n",
       "      <td>3.476083e-07</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-7.013003e-06</td>\n",
       "      <td>-2.157351e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-6.576047e-06</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-1.798475e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-1.934877e-06</td>\n",
       "      <td>-1.137100e-05</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-1.240701e-06</td>\n",
       "      <td>1.164552e-07</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-8.816946e-06</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-7.726917e-07</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-6.883572e-06</td>\n",
       "      <td>-8.592138e-06</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-1.816056e-05</td>\n",
       "      <td>-1.805600e-05</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-1.174795e-05</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-1.764283e-05</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-2.481684e-05</td>\n",
       "      <td>-2.555257e-05</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-1.423888e-05</td>\n",
       "      <td>-1.432938e-05</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-6.548170e-07</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-9.832716e-06</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-4.043191e-06</td>\n",
       "      <td>2.869733e-06</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9661</th>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-7.960396e-06</td>\n",
       "      <td>-2.564423e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>-2.085907e-05</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-4.158658e-06</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-1.726886e-05</td>\n",
       "      <td>-1.799114e-05</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9662</th>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-1.471854e-05</td>\n",
       "      <td>-1.137177e-05</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-1.593974e-05</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-2.306823e-05</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-2.388908e-05</td>\n",
       "      <td>-2.187267e-05</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9663</th>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-1.854585e-05</td>\n",
       "      <td>-1.930124e-05</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-2.685072e-05</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-1.071035e-05</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-1.055121e-05</td>\n",
       "      <td>-9.590154e-06</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9664</th>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-3.252550e-05</td>\n",
       "      <td>-3.268349e-05</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>-2.396195e-05</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-1.363439e-05</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-1.254522e-05</td>\n",
       "      <td>-9.661315e-06</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9665</th>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-4.212195e-07</td>\n",
       "      <td>-8.669054e-06</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>4.816951e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-2.986949e-05</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-1.807786e-05</td>\n",
       "      <td>-3.532270e-05</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9666 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2             3             4         5  \\\n",
       "0    -0.000007 -0.000004 -0.000001 -4.339751e-06 -9.557706e-06 -0.000010   \n",
       "1    -0.000010 -0.000015 -0.000011 -7.013003e-06 -2.157351e-06 -0.000003   \n",
       "2    -0.000001 -0.000003 -0.000001 -1.240701e-06  1.164552e-07 -0.000002   \n",
       "3    -0.000019 -0.000019 -0.000018 -1.816056e-05 -1.805600e-05 -0.000019   \n",
       "4     0.000001 -0.000005 -0.000010 -1.423888e-05 -1.432938e-05 -0.000013   \n",
       "...        ...       ...       ...           ...           ...       ...   \n",
       "9661 -0.000024 -0.000020 -0.000020 -7.960396e-06 -2.564423e-05  0.000004   \n",
       "9662 -0.000013 -0.000014 -0.000014 -1.471854e-05 -1.137177e-05 -0.000013   \n",
       "9663 -0.000007 -0.000009 -0.000013 -1.854585e-05 -1.930124e-05 -0.000023   \n",
       "9664 -0.000018 -0.000024 -0.000029 -3.252550e-05 -3.268349e-05 -0.000032   \n",
       "9665  0.000001  0.000007  0.000003 -4.212195e-07 -8.669054e-06 -0.000010   \n",
       "\n",
       "             6         7             8         9  ...       119       120  \\\n",
       "0    -0.000008 -0.000005 -2.562251e-06 -0.000003  ... -0.000027 -0.000026   \n",
       "1    -0.000006 -0.000007 -6.576047e-06 -0.000009  ... -0.000013 -0.000007   \n",
       "2    -0.000003 -0.000006 -8.816946e-06 -0.000008  ... -0.000003 -0.000001   \n",
       "3    -0.000016 -0.000015 -1.174795e-05 -0.000009  ... -0.000014 -0.000015   \n",
       "4    -0.000006  0.000002 -6.548170e-07 -0.000006  ... -0.000027 -0.000017   \n",
       "...        ...       ...           ...       ...  ...       ...       ...   \n",
       "9661  0.000011 -0.000033 -2.085907e-05 -0.000024  ... -0.000012 -0.000003   \n",
       "9662 -0.000013 -0.000012 -1.593974e-05 -0.000013  ... -0.000023 -0.000023   \n",
       "9663 -0.000026 -0.000026 -2.685072e-05 -0.000027  ... -0.000012 -0.000010   \n",
       "9664 -0.000029 -0.000025 -2.396195e-05 -0.000024  ... -0.000013 -0.000014   \n",
       "9665 -0.000008 -0.000003  4.816951e-06  0.000007  ... -0.000028 -0.000014   \n",
       "\n",
       "               121       122       123           124           125       126  \\\n",
       "0    -2.090641e-05 -0.000016 -0.000002 -7.125459e-07  3.476083e-07 -0.000009   \n",
       "1    -1.798475e-06  0.000006  0.000003 -1.934877e-06 -1.137100e-05 -0.000020   \n",
       "2    -7.726917e-07 -0.000003 -0.000007 -6.883572e-06 -8.592138e-06 -0.000012   \n",
       "3    -1.764283e-05 -0.000020 -0.000021 -2.481684e-05 -2.555257e-05 -0.000027   \n",
       "4    -9.832716e-06 -0.000009 -0.000005 -4.043191e-06  2.869733e-06 -0.000002   \n",
       "...            ...       ...       ...           ...           ...       ...   \n",
       "9661 -4.158658e-06 -0.000013 -0.000021 -1.726886e-05 -1.799114e-05 -0.000022   \n",
       "9662 -2.306823e-05 -0.000023 -0.000021 -2.388908e-05 -2.187267e-05 -0.000022   \n",
       "9663 -1.071035e-05 -0.000011 -0.000012 -1.055121e-05 -9.590154e-06 -0.000009   \n",
       "9664 -1.363439e-05 -0.000012 -0.000007 -1.254522e-05 -9.661315e-06 -0.000019   \n",
       "9665 -2.986949e-05 -0.000015 -0.000034 -1.807786e-05 -3.532270e-05 -0.000017   \n",
       "\n",
       "           127  OUTPUT_LABEL  \n",
       "0    -0.000012             0  \n",
       "1    -0.000021             0  \n",
       "2    -0.000014             0  \n",
       "3    -0.000026             0  \n",
       "4    -0.000003             1  \n",
       "...        ...           ...  \n",
       "9661 -0.000025             1  \n",
       "9662 -0.000021             0  \n",
       "9663 -0.000007             0  \n",
       "9664 -0.000014             1  \n",
       "9665 -0.000032             0  \n",
       "\n",
       "[9666 rows x 129 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_compressed = x_train.squeeze(-1)\n",
    "num_samples = x_train_compressed.shape[0]\n",
    "num_channels = x_train_compressed.shape[1]\n",
    "num_data_points = x_train_compressed.shape[2]\n",
    "x_train_combo = np.zeros((num_samples, num_data_points))\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    x_train_combo[idx] = x_train_compressed[idx].mean(axis=0)\n",
    "\n",
    "x_train_df = pd.DataFrame(x_train_combo)\n",
    "x_train_df.loc[:, 'OUTPUT_LABEL'] = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prevalence of the positive class in training set: 0.175\n",
      "prevalence of the positive class in dev set: 0.177\n"
     ]
    }
   ],
   "source": [
    "def calc_prevalence(y_actual):\n",
    "    # this function calculates the prevalence of the positive class (label = 1)\n",
    "    return sum(y_actual) / len(y_actual)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"prevalence of the positive class in training set: %.3f\"\n",
    "    % calc_prevalence(x_train_df[\"OUTPUT_LABEL\"].values)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"prevalence of the positive class in dev set: %.3f\"\n",
    "    % calc_prevalence(x_dev_df[\"OUTPUT_LABEL\"].values)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = x_train_df.append(x_dev_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>OUTPUT_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.806897e-05</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>2.694147e-05</td>\n",
       "      <td>2.631345e-05</td>\n",
       "      <td>3.342525e-05</td>\n",
       "      <td>2.893065e-05</td>\n",
       "      <td>3.049098e-05</td>\n",
       "      <td>2.887583e-05</td>\n",
       "      <td>1.706594e-05</td>\n",
       "      <td>1.142987e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>3.588595e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>5.537355e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.586976e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-3.815148e-06</td>\n",
       "      <td>-4.872682e-06</td>\n",
       "      <td>-5.517447e-06</td>\n",
       "      <td>-6.604569e-06</td>\n",
       "      <td>-7.234552e-06</td>\n",
       "      <td>-7.359232e-06</td>\n",
       "      <td>-7.998945e-06</td>\n",
       "      <td>-8.391219e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>5.451828e-06</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4.949192e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.305686e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>9.457199e-07</td>\n",
       "      <td>1.710799e-06</td>\n",
       "      <td>1.797817e-07</td>\n",
       "      <td>3.124812e-07</td>\n",
       "      <td>-4.158639e-07</td>\n",
       "      <td>-9.511383e-07</td>\n",
       "      <td>-3.395080e-07</td>\n",
       "      <td>-9.281217e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1.577728e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.239872e-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.212032e-07</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.759689e-06</td>\n",
       "      <td>-3.015222e-06</td>\n",
       "      <td>-6.349563e-06</td>\n",
       "      <td>1.551572e-06</td>\n",
       "      <td>-9.040650e-07</td>\n",
       "      <td>1.054309e-06</td>\n",
       "      <td>1.000005e-05</td>\n",
       "      <td>8.096936e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-9.365033e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.042715e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.826868e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-1.890729e-06</td>\n",
       "      <td>-5.309772e-06</td>\n",
       "      <td>-7.681002e-06</td>\n",
       "      <td>-7.751689e-06</td>\n",
       "      <td>-4.373733e-06</td>\n",
       "      <td>-2.505600e-06</td>\n",
       "      <td>-3.381539e-06</td>\n",
       "      <td>-4.035801e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4.007283e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>9.012014e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54887</th>\n",
       "      <td>-2.380721e-05</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-1.992690e-05</td>\n",
       "      <td>-7.960396e-06</td>\n",
       "      <td>-2.564423e-05</td>\n",
       "      <td>4.145606e-06</td>\n",
       "      <td>1.059705e-05</td>\n",
       "      <td>-3.260366e-05</td>\n",
       "      <td>-2.085907e-05</td>\n",
       "      <td>-2.422808e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-4.158658e-06</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-2.515988e-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54888</th>\n",
       "      <td>-1.267327e-05</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-1.394527e-05</td>\n",
       "      <td>-1.471854e-05</td>\n",
       "      <td>-1.137177e-05</td>\n",
       "      <td>-1.339988e-05</td>\n",
       "      <td>-1.298330e-05</td>\n",
       "      <td>-1.240143e-05</td>\n",
       "      <td>-1.593974e-05</td>\n",
       "      <td>-1.314995e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-2.306823e-05</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-2.106025e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54889</th>\n",
       "      <td>-6.756249e-06</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-1.256188e-05</td>\n",
       "      <td>-1.854585e-05</td>\n",
       "      <td>-1.930124e-05</td>\n",
       "      <td>-2.287567e-05</td>\n",
       "      <td>-2.552521e-05</td>\n",
       "      <td>-2.587479e-05</td>\n",
       "      <td>-2.685072e-05</td>\n",
       "      <td>-2.708476e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-1.071035e-05</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-6.807722e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54890</th>\n",
       "      <td>-1.787499e-05</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-2.850391e-05</td>\n",
       "      <td>-3.252550e-05</td>\n",
       "      <td>-3.268349e-05</td>\n",
       "      <td>-3.229526e-05</td>\n",
       "      <td>-2.867846e-05</td>\n",
       "      <td>-2.549913e-05</td>\n",
       "      <td>-2.396195e-05</td>\n",
       "      <td>-2.387907e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-1.363439e-05</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-1.436840e-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54891</th>\n",
       "      <td>1.314403e-06</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.347723e-06</td>\n",
       "      <td>-4.212195e-07</td>\n",
       "      <td>-8.669054e-06</td>\n",
       "      <td>-1.033206e-05</td>\n",
       "      <td>-8.248877e-06</td>\n",
       "      <td>-3.047820e-06</td>\n",
       "      <td>4.816951e-06</td>\n",
       "      <td>7.119318e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-2.986949e-05</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-3.158031e-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54892 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1             2             3             4  \\\n",
       "0      2.806897e-05  0.000024  2.694147e-05  2.631345e-05  3.342525e-05   \n",
       "1     -3.586976e-06 -0.000003 -3.815148e-06 -4.872682e-06 -5.517447e-06   \n",
       "2     -2.305686e-06 -0.000003  9.457199e-07  1.710799e-06  1.797817e-07   \n",
       "3      6.212032e-07  0.000002  2.759689e-06 -3.015222e-06 -6.349563e-06   \n",
       "4     -4.826868e-06 -0.000003 -1.890729e-06 -5.309772e-06 -7.681002e-06   \n",
       "...             ...       ...           ...           ...           ...   \n",
       "54887 -2.380721e-05 -0.000020 -1.992690e-05 -7.960396e-06 -2.564423e-05   \n",
       "54888 -1.267327e-05 -0.000014 -1.394527e-05 -1.471854e-05 -1.137177e-05   \n",
       "54889 -6.756249e-06 -0.000009 -1.256188e-05 -1.854585e-05 -1.930124e-05   \n",
       "54890 -1.787499e-05 -0.000024 -2.850391e-05 -3.252550e-05 -3.268349e-05   \n",
       "54891  1.314403e-06  0.000007  3.347723e-06 -4.212195e-07 -8.669054e-06   \n",
       "\n",
       "                  5             6             7             8             9  \\\n",
       "0      2.893065e-05  3.049098e-05  2.887583e-05  1.706594e-05  1.142987e-05   \n",
       "1     -6.604569e-06 -7.234552e-06 -7.359232e-06 -7.998945e-06 -8.391219e-06   \n",
       "2      3.124812e-07 -4.158639e-07 -9.511383e-07 -3.395080e-07 -9.281217e-07   \n",
       "3      1.551572e-06 -9.040650e-07  1.054309e-06  1.000005e-05  8.096936e-06   \n",
       "4     -7.751689e-06 -4.373733e-06 -2.505600e-06 -3.381539e-06 -4.035801e-06   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "54887  4.145606e-06  1.059705e-05 -3.260366e-05 -2.085907e-05 -2.422808e-05   \n",
       "54888 -1.339988e-05 -1.298330e-05 -1.240143e-05 -1.593974e-05 -1.314995e-05   \n",
       "54889 -2.287567e-05 -2.552521e-05 -2.587479e-05 -2.685072e-05 -2.708476e-05   \n",
       "54890 -3.229526e-05 -2.867846e-05 -2.549913e-05 -2.396195e-05 -2.387907e-05   \n",
       "54891 -1.033206e-05 -8.248877e-06 -3.047820e-06  4.816951e-06  7.119318e-06   \n",
       "\n",
       "       ...       119       120           121       122       123       124  \\\n",
       "0      ...  0.000030  0.000010  3.588595e-06  0.000002  0.000018  0.000016   \n",
       "1      ...  0.000006  0.000005  5.451828e-06  0.000006  0.000007  0.000006   \n",
       "2      ... -0.000004 -0.000003  1.577728e-06  0.000002  0.000002  0.000003   \n",
       "3      ...  0.000010  0.000007 -9.365033e-07  0.000005  0.000018  0.000008   \n",
       "4      ...  0.000002  0.000004  4.007283e-06  0.000005  0.000004  0.000003   \n",
       "...    ...       ...       ...           ...       ...       ...       ...   \n",
       "54887  ... -0.000012 -0.000003 -4.158658e-06 -0.000013 -0.000021 -0.000017   \n",
       "54888  ... -0.000023 -0.000023 -2.306823e-05 -0.000023 -0.000021 -0.000024   \n",
       "54889  ... -0.000012 -0.000010 -1.071035e-05 -0.000011 -0.000012 -0.000011   \n",
       "54890  ... -0.000013 -0.000014 -1.363439e-05 -0.000012 -0.000007 -0.000013   \n",
       "54891  ... -0.000028 -0.000014 -2.986949e-05 -0.000015 -0.000034 -0.000018   \n",
       "\n",
       "            125       126           127  OUTPUT_LABEL  \n",
       "0      0.000015  0.000042  5.537355e-05             0  \n",
       "1      0.000006  0.000005  4.949192e-06             0  \n",
       "2      0.000003  0.000002  2.239872e-06             1  \n",
       "3      0.000008  0.000006  1.042715e-05             0  \n",
       "4      0.000003  0.000004  9.012014e-07             1  \n",
       "...         ...       ...           ...           ...  \n",
       "54887 -0.000018 -0.000022 -2.515988e-05             1  \n",
       "54888 -0.000022 -0.000022 -2.106025e-05             0  \n",
       "54889 -0.000010 -0.000009 -6.807722e-06             0  \n",
       "54890 -0.000010 -0.000019 -1.436840e-05             1  \n",
       "54891 -0.000035 -0.000017 -3.158031e-05             0  \n",
       "\n",
       "[54892 rows x 129 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "collist = combined_data.columns.tolist()\n",
    "cols_input = collist[0:128]\n",
    "df_data = combined_data[cols_input + [\"OUTPUT_LABEL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# check for duplicated columns in cols_input\n",
    "dup_cols = set([x for x in cols_input if cols_input.count(x) > 1])\n",
    "print(dup_cols)\n",
    "assert len(dup_cols) == 0, \"you have duplicated columns in cols_input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# check for duplicated columns in df_data\n",
    "cols_df_data = list(df_data.columns)\n",
    "dup_cols = set([x for x in cols_df_data if cols_df_data.count(x) > 1])\n",
    "print(dup_cols)\n",
    "assert len(dup_cols) == 0,'you have duplicated columns in df_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the size of df_data makes sense\n",
    "assert (len(cols_input) + 1) == len(\n",
    "    df_data.columns\n",
    "), \"issue with dimensions of df_data or cols_input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_data.sample(n=len(df_data))\n",
    "df_data = df_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation/Test Split Size: 0.3\n"
     ]
    }
   ],
   "source": [
    "df_valid_test = df_data.sample(frac=0.3)\n",
    "print(\"Validation/Test Split Size: %.1f\" % (len(df_valid_test) / len(df_data)))\n",
    "\n",
    "df_test = df_valid_test.sample(frac=0.5)\n",
    "df_valid = df_valid_test.drop(df_test.index)\n",
    "\n",
    "df_train_all = df_data.drop(df_valid_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prevalence(n = 8234):0.181\n",
      "Valid prevalence(n = 8234):0.176\n",
      "Train all prevalence(n = 38424):0.174\n"
     ]
    }
   ],
   "source": [
    "# check the prevalence of each\n",
    "print(\n",
    "    \"Test prevalence(n = %d):%.3f\"\n",
    "    % (len(df_test), calc_prevalence(df_test.OUTPUT_LABEL.values))\n",
    ")\n",
    "print(\n",
    "    \"Valid prevalence(n = %d):%.3f\"\n",
    "    % (len(df_valid), calc_prevalence(df_valid.OUTPUT_LABEL.values))\n",
    ")\n",
    "print(\n",
    "    \"Train all prevalence(n = %d):%.3f\"\n",
    "    % (len(df_train_all), calc_prevalence(df_train_all.OUTPUT_LABEL.values))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all samples (n = 54892)\n"
     ]
    }
   ],
   "source": [
    "print('all samples (n = %d)'%len(df_data))\n",
    "assert len(df_data) == (len(df_test)+len(df_valid)+len(df_train_all)),'math didnt work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train balanced prevalence(n = 13358):0.500\n"
     ]
    }
   ],
   "source": [
    "rows_pos = df_train_all.OUTPUT_LABEL == 1\n",
    "df_train_pos = df_train_all.loc[rows_pos]\n",
    "df_train_neg = df_train_all.loc[~rows_pos]\n",
    "\n",
    "n = np.min([len(df_train_pos), len(df_train_neg)])\n",
    "\n",
    "df_train = pd.concat([df_train_pos.sample(n=n, random_state=69), df_train_neg.sample(n=n, random_state=69)], axis=0, ignore_index=True)\n",
    "\n",
    "df_train = df_train.sample(n=len(df_train), random_state=69).reset_index(drop=True)\n",
    "\n",
    "print('Train balanced prevalence(n = %d):%.3f'%(len(df_train), calc_prevalence(df_train.OUTPUT_LABEL.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all.to_csv('df_train_all_tusz.csv',index=False)\n",
    "df_train.to_csv('df_train_tusz.csv',index=False)\n",
    "df_valid.to_csv('df_valid_tusz.csv',index=False)\n",
    "df_test.to_csv('df_test_tusz.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(cols_input, open('cols_input_tusz.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training All shapes: (38424, 128)\n",
      "Training shapes: (13358, 128) (13358,)\n",
      "Validation shapes: (8234, 128) (8234,)\n"
     ]
    }
   ],
   "source": [
    "# create the X and y matrices\n",
    "X_train = df_train[cols_input].values\n",
    "X_train_all = df_train_all[cols_input].values\n",
    "X_valid = df_valid[cols_input].values\n",
    "\n",
    "y_train = df_train['OUTPUT_LABEL'].values\n",
    "y_valid = df_valid['OUTPUT_LABEL'].values\n",
    "\n",
    "print('Training All shapes:',X_train_all.shape)\n",
    "print('Training shapes:',X_train.shape, y_train.shape)\n",
    "print('Validation shapes:',X_valid.shape, y_valid.shape)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler  = StandardScaler()\n",
    "scaler.fit(X_train_all)\n",
    "\n",
    "scalerfile = 'scaler_tusz.sav'\n",
    "pickle.dump(scaler, open(scalerfile, 'wb'))\n",
    "scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "\n",
    "# transform our data matrices\n",
    "X_train_tf = scaler.transform(X_train)\n",
    "X_valid_tf = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "\n",
    "def print_report(y_actual, y_pred, thresh):\n",
    "    \n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print('prevalence:%.3f'%calc_prevalence(y_actual))\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13358, 128)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Training:\n",
      "AUC:0.758\n",
      "accuracy:0.690\n",
      "recall:0.694\n",
      "precision:0.689\n",
      "specificity:0.678\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.761\n",
      "accuracy:0.684\n",
      "recall:0.708\n",
      "precision:0.320\n",
      "specificity:0.670\n",
      "prevalence:0.176\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors = 100)\n",
    "knn.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = knn.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = knn.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('KNN')\n",
    "print('Training:')\n",
    "knn_train_auc, knn_train_accuracy, knn_train_recall, \\\n",
    "    knn_train_precision, knn_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "knn_valid_auc, knn_valid_accuracy, knn_valid_recall, \\\n",
    "    knn_valid_precision, knn_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78, 0.29, 0.46, 0.3 , 0.54, 0.48, 0.35, 0.49, 0.58, 0.2 , 0.33,\n",
       "       0.7 , 0.2 , 0.36, 0.5 , 0.15, 0.83, 0.29, 0.65, 0.2 , 0.57, 0.64,\n",
       "       0.01, 0.25, 0.82, 0.76, 0.06, 0.31, 0.15, 0.4 , 0.35, 0.  , 0.88,\n",
       "       0.33, 0.4 , 0.84, 0.66, 0.75, 0.33, 0.39, 0.05, 0.74, 0.81, 0.01,\n",
       "       0.44, 0.16, 0.88, 0.2 , 0.02, 0.87, 0.62, 0.33, 0.51, 0.21, 0.25,\n",
       "       0.73, 0.14, 0.17, 0.06, 0.16, 0.8 , 0.04, 0.32, 0.  , 0.28, 0.88,\n",
       "       0.25, 0.82, 0.62, 0.18, 0.12, 0.23, 0.9 , 0.81, 0.19, 0.18, 0.41,\n",
       "       0.26, 0.56, 0.61, 0.1 , 0.61, 0.74, 0.12, 0.25, 0.73, 0.19, 0.28,\n",
       "       0.64, 0.38, 0.19, 0.71, 0.16, 0.19, 0.34, 0.61, 0.41, 0.88, 0.88,\n",
       "       0.26])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_preds[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(knn, open('knn_classifier.pkl', 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Training:\n",
      "AUC:0.566\n",
      "accuracy:0.543\n",
      "recall:0.604\n",
      "precision:0.538\n",
      "specificity:0.482\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.530\n",
      "accuracy:0.473\n",
      "recall:0.575\n",
      "precision:0.183\n",
      "specificity:0.452\n",
      "prevalence:0.176\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state = 69)\n",
    "lr.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = lr.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = lr.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Logistic Regression')\n",
    "print('Training:')\n",
    "lr_train_auc, lr_train_accuracy, lr_train_recall, \\\n",
    "    lr_train_precision, lr_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "lr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n",
    "    lr_valid_precision, lr_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDC\n",
      "Training:\n",
      "AUC:0.527\n",
      "accuracy:0.530\n",
      "recall:0.643\n",
      "precision:0.525\n",
      "specificity:0.418\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.511\n",
      "accuracy:0.443\n",
      "recall:0.634\n",
      "precision:0.185\n",
      "specificity:0.402\n",
      "prevalence:0.176\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "SGDC = SGDClassifier(loss = 'log',alpha = 0.1)\n",
    "SGDC.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = SGDC.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = SGDC.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('SGDC')\n",
    "print('Training:')\n",
    "SGDC_train_auc, SGDC_train_accuracy, SGDC_train_recall, \\\n",
    "    SGDC_train_precision, SGDC_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "SGDC_valid_auc, SGDC_valid_accuracy, SGDC_valid_recall, \\\n",
    "    SGDC_valid_precision, SGDC_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Training:\n",
      "AUC:0.527\n",
      "accuracy:0.524\n",
      "recall:0.963\n",
      "precision:0.513\n",
      "specificity:0.084\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.526\n",
      "accuracy:0.236\n",
      "recall:0.965\n",
      "precision:0.183\n",
      "specificity:0.080\n",
      "prevalence:0.176\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = nb.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = nb.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Naive Bayes')\n",
    "print('Training:')\n",
    "nb_train_auc, nb_train_accuracy, nb_train_recall, nb_train_precision, \\\n",
    "nb_train_specificity =print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "nb_valid_auc, nb_valid_accuracy, nb_valid_recall, nb_valid_precision, \\\n",
    "nb_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, random_state=69)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth = 10, random_state = 69)\n",
    "tree.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Training:\n",
      "AUC:0.877\n",
      "accuracy:0.799\n",
      "recall:0.806\n",
      "precision:0.794\n",
      "specificity:0.790\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.693\n",
      "accuracy:0.656\n",
      "recall:0.679\n",
      "precision:0.294\n",
      "specificity:0.650\n",
      "prevalence:0.176\n",
      " \n"
     ]
    }
   ],
   "source": [
    "y_train_preds = tree.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = tree.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Decision Tree')\n",
    "print('Training:')\n",
    "tree_train_auc, tree_train_accuracy, tree_train_recall, tree_train_precision, \\\n",
    "tree_train_specificity =print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "tree_valid_auc, tree_valid_accuracy, tree_valid_recall, tree_valid_precision, \\\n",
    "tree_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Training:\n",
      "AUC:0.820\n",
      "accuracy:0.742\n",
      "recall:0.744\n",
      "precision:0.741\n",
      "specificity:0.740\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.775\n",
      "accuracy:0.702\n",
      "recall:0.706\n",
      "precision:0.335\n",
      "specificity:0.701\n",
      "prevalence:0.176\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth = 6, random_state = 69)\n",
    "rf.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = rf.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = rf.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Random Forest')\n",
    "print('Training:')\n",
    "rf_train_auc, rf_train_accuracy, rf_train_recall, rf_train_precision, \\\n",
    "rf_train_specificity =print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "rf_valid_auc, rf_valid_accuracy, rf_valid_recall, rf_valid_precision, \\\n",
    "rf_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier\n",
      "Training:\n",
      "AUC:0.943\n",
      "accuracy:0.871\n",
      "recall:0.884\n",
      "precision:0.861\n",
      "specificity:0.857\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.751\n",
      "accuracy:0.677\n",
      "recall:0.716\n",
      "precision:0.316\n",
      "specificity:0.669\n",
      "prevalence:0.176\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=1.0, max_depth=3, random_state=69)\n",
    "gbc.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = gbc.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = gbc.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Gradient Boosting Classifier')\n",
    "print('Training:')\n",
    "gbc_train_auc, gbc_train_accuracy, gbc_train_recall, gbc_train_precision, \\\n",
    "gbc_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "gbc_valid_auc, gbc_valid_accuracy, gbc_valid_recall, gbc_valid_precision, \\\n",
    "gbc_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Classifier\n",
      "Training:\n",
      "AUC:0.999\n",
      "accuracy:0.988\n",
      "recall:0.991\n",
      "precision:0.986\n",
      "specificity:0.986\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.817\n",
      "accuracy:0.744\n",
      "recall:0.739\n",
      "precision:0.382\n",
      "specificity:0.745\n",
      "prevalence:0.176\n",
      " \n"
     ]
    }
   ],
   "source": [
    "etc = ExtraTreesClassifier(bootstrap=False, criterion=\"entropy\", max_features=1.0,\n",
    "                           min_samples_leaf=3, min_samples_split=20, n_estimators=100)\n",
    "etc.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = etc.predict_proba(X_train_tf)[:, 1]\n",
    "y_valid_preds = etc.predict_proba(X_valid_tf)[:, 1]\n",
    "\n",
    "print('Extra Trees Classifier')\n",
    "print('Training:')\n",
    "etc_train_auc, etc_train_accuracy, etc_train_recall, etc_train_precision, \\\n",
    "etc_train_specificity = print_report(y_train, y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "etc_valid_auc, etc_valid_accuracy, etc_valid_recall, etc_valid_precision, \\\n",
    "etc_valid_specificity = print_report(y_valid, y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:29:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Xtreme Gradient Boosting Classifier\n",
      "Training:\n",
      "AUC:0.998\n",
      "accuracy:0.982\n",
      "recall:0.991\n",
      "precision:0.974\n",
      "specificity:0.973\n",
      "prevalence:0.500\n",
      " \n",
      "Validation:\n",
      "AUC:0.795\n",
      "accuracy:0.713\n",
      "recall:0.742\n",
      "precision:0.351\n",
      "specificity:0.707\n",
      "prevalence:0.176\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X_train_tf, y_train)\n",
    "\n",
    "y_train_preds = xgbc.predict_proba(X_train_tf)[:,1]\n",
    "y_valid_preds = xgbc.predict_proba(X_valid_tf)[:,1]\n",
    "\n",
    "print('Xtreme Gradient Boosting Classifier')\n",
    "print('Training:')\n",
    "xgbc_train_auc, xgbc_train_accuracy, xgbc_train_recall, xgbc_train_precision, \\\n",
    "xgbc_train_specificity = print_report(y_train,y_train_preds, thresh)\n",
    "print('Validation:')\n",
    "xgbc_valid_auc, xgbc_valid_accuracy, xgbc_valid_recall, xgbc_valid_precision, \\\n",
    "xgbc_valid_specificity = print_report(y_valid,y_valid_preds, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({'classifier':['KNN','KNN','LR','LR','SGD','SGD','NB','NB','DT','DT','RF','RF','GB','GB','XGBC','XGBC','ETC','ETC'],\n",
    "                           'data_set':['train','valid']*9,\n",
    "                          'auc':[knn_train_auc, knn_valid_auc,lr_train_auc,lr_valid_auc,SGDC_train_auc,SGDC_valid_auc,nb_train_auc,nb_valid_auc,tree_train_auc,tree_valid_auc,rf_train_auc,rf_valid_auc,gbc_train_auc,gbc_valid_auc,xgbc_train_auc,xgbc_valid_auc,etc_train_auc,etc_valid_auc],\n",
    "                          'accuracy':[knn_train_accuracy, knn_valid_accuracy,lr_train_accuracy,lr_valid_accuracy,SGDC_train_accuracy, SGDC_valid_accuracy,nb_train_accuracy,nb_valid_accuracy,tree_train_accuracy,tree_valid_accuracy,rf_train_accuracy,rf_valid_accuracy,gbc_train_accuracy,gbc_valid_accuracy,xgbc_train_accuracy,xgbc_valid_accuracy,etc_train_accuracy,etc_valid_accuracy],\n",
    "                          'recall':[knn_train_recall, knn_valid_recall,lr_train_recall,lr_valid_recall,SGDC_train_recall,SGDC_valid_recall,nb_train_recall,nb_valid_recall,tree_train_recall,tree_valid_recall,rf_train_recall,rf_valid_recall,gbc_train_recall,gbc_valid_recall,xgbc_train_recall,xgbc_valid_recall,etc_train_recall,etc_valid_recall],\n",
    "                          'precision':[knn_train_precision, knn_valid_precision,lr_train_precision,lr_valid_precision,SGDC_train_precision,SGDC_valid_precision,nb_train_precision,nb_valid_precision,tree_train_precision,tree_valid_precision,rf_train_precision,rf_valid_precision,gbc_train_precision,gbc_valid_precision,xgbc_train_precision,xgbc_valid_precision,etc_train_precision,etc_valid_precision],\n",
    "                          'specificity':[knn_train_specificity, knn_valid_specificity,lr_train_specificity,lr_valid_specificity,SGDC_train_specificity,SGDC_valid_specificity,nb_train_specificity,nb_valid_specificity,tree_train_specificity,tree_valid_specificity,rf_train_specificity,rf_valid_specificity,gbc_train_specificity,gbc_valid_specificity,xgbc_train_specificity,xgbc_valid_specificity,etc_train_specificity,etc_valid_specificity]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd301525160>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABD0AAAHqCAYAAAAH9zVIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABAXklEQVR4nO3deZxWZf0//hfLDJuiooKyVi6QC4sIpGImmqm5kVSKSi6kpkIuqJhG4Q4KaRji2kfENVMStc1ySSv9IHxzR6U0JREIlURggJnfH/6YTyOLIHDPcHw+H495POY+5zrnvO/7mrnP3K8513XqVVVVVQUAAACgYOrXdgEAAAAA64PQAwAAACgkoQcAAABQSEIPAAAAoJAa1nYB61NlZWXmz5+fsrKy1KtXr7bLAQAAYB2pqqrK4sWL06xZs9Sv7//5rFihQ4/58+fnlVdeqe0yAAAAWE+23377bLzxxrVdxhqZN29eZs2alcWLF9d2KRu0srKytGzZMs2bN19pm0KHHmVlZUk++iUoLy+v5WoAAABYVyoqKvLKK69Uf+7bUMybNy/vvPNO2rRpkyZNmhiV8ClVVVVlwYIFmTFjRpKsNPgodOix7IenvLw8jRo1quVqAAAAWNc2tNBg1qxZadOmTZo2bVrbpWzQ6tWrl6ZNm6ZNmzb517/+tdLQw8AnAAAAKJHFixenSZMmtV1GYTRp0mSVw4SEHgAAAFBCG9rVKXXZJ72WQg8AAACgkIQeAAAAQCEJPQAAAKCWVSxeusEc97XXXsujjz76qY85dOjQHHvssZ96+zVR6Lu3AAAAwIagvKxB+p9zW8mPe/vIo9Z4m1NOOSUHH3xwvvKVr3yqY55//vmprKz8VNuuKaEHAAAAsNqqqqrWavuNN954HVXyyQxvAQAAAFbLMccck3/+85+55ppr0qdPn/Tp0ycjRozI1772tXzpS1/KCy+8kLfeeiuDBw9Or169suOOO6ZPnz658cYbq/fx38Nbnnrqqey88855+OGHs//++6dr16751re+lcmTJ6+TeoUeAAAAwGoZM2ZM2rRpk+OPPz733HNPkuSOO+7IRRddlOuuuy5f/OIX873vfS8VFRUZP358HnrooRx66KG54oor8tJLL61wn4sXL84111yTiy++OLfffnuS5Ac/+MFaX1GSCD0AAACA1bTpppumQYMGadq0aVq0aJEk6dOnT3r27JkuXbqkoqIiffv2zfDhw9OxY8d06NAhp512WurXr59p06atcJ9VVVU544wzsuuuu2aHHXbIiSeemDfeeCPvvvvuWtdrTg8AAADgU2vXrl31940bN87RRx+dhx56KM8++2zeeOONvPTSS6msrFzl5KWf//znq79fNufH4sWL17o2oQcAAADwqTVq1Kj6+w8//DD9+/fP0qVL87WvfS29evVKly5dsvfee69yH+Xl5cstWxfDW4QeAAAAwGqrV6/eStc9/fTTeemll/LUU09l0003TZL8/e9/T2Vl5ToJMdZUrc3pMWzYsJx//vmrbPPcc8/liCOOSJcuXbLffvtl4sSJpSkOAAAAWKFmzZrl9ddfzzvvvLPcumXzfEyaNCkzZszIX/7yl5x++ulJkoqKilKWmaQWrvSoqqrKT3/609x1113p16/fStvNnTs3AwcOzEEHHZRLLrkkf/7zn3P++edniy22SO/evUtYMQAAAKxfFYuX5vaRR9XKccvLGqzRNscee2wuvvjiPPHEE2nSpEmNdZ07d84555yTG264IVdccUVat26dfv365fHHH89zzz2XI488cl2W/4nqVZXw+pI333wzP/jBD/Lqq6+mSZMm2X333XPJJZessO11112Xu+++O7///e9Tv/5HF6Scd955eeedd3LzzTev1vEWLVqU559/PjvttFONMUYAAABs2DbUz3svvfRSvvjFL9Z2GYWyqte0pMNbpk6dmnbt2mXSpElp27btKttOnjw5PXr0qA48kqRnz56ZMmXKKmd8BQAAAEhKPLzlkEMOySGHHLJabWfOnJkddtihxrKWLVtmwYIFee+996rHCQEAAACsSJ29e8vChQuXu2XNssdrOvnJ888/v87qAgAAWB1f/OKOadq0cW2XsVIffrgwL730Qm2XAetVnQ09GjduvFy4sezxxydK+SQb2hgvAAD4tD7NpISlVNfrW9f6n3NbbZewUrePPCrdu3ev7TI+tWVzesCq1NnQY6uttsrs2bNrLJs1a1aaNm2ajTfeuJaqAgCAuq28rEGd/6ANUColnch0TXTv3j2TJ0/Of99c5qmnnsouu+xSY3JTAAAAgBWpM+lBRUVFZs+eXT2EpV+/fpk7d25+9KMfZfr06bn11lvzwAMPZODAgbVcKQAAALAhqDOhx9SpU9O7d+9MnTo1SbLFFlvkxhtvzIsvvpjDDjssEyZMyIgRI7LbbrvVcqUAAADAhqDW5vS49dZbazzu1atXpk2bVmNZ165dc88995SyLAAAAKAg6syVHgAAAPBZVblk8WfquKVSZ+/eAgAAAJ8V9RuW5ZmRpZ/Dsvs5N5bsWE899VQGDBiQxx57LFtttVX69OmTfv365ZRTTllh+/PPPz///Oc/lxspsiaEHgAAAEDJ3XPPPWncuPF6PYbQAwAAACi5Fi1arPdjmNMDAAAAWC3nnntujjnmmBrLnn322XTs2DH/+Mc/Mnbs2Oy3337Zaaedsuuuu2bQoEGZO3fuCvfVp0+fjB07tvrxbbfdlj59+qRLly4ZMmRIFi5cuNb1Cj0AAACA1XLYYYdl8uTJeeedd6qXTZo0Kd26dcsf//jHjB8/PhdccEF++9vfZtSoUXnmmWdy7bXXfuJ+J06cmMsuuywnn3xy7rvvvmy11VZ54IEH1rpeoQcAAACwWr70pS9lq622ykMPPZQkWbp0aX7961/nsMMOy+c///mMGDEiX/7yl9OmTZvstdde2XPPPfPKK6984n5vu+22HHLIIfnWt76VL3zhCxkyZEh23nnnta5X6AEAAACslnr16uWQQw6pvgrjL3/5S95///0ceOCB6dOnT5o3b56f/OQnGTx4cA466KDcf//9qays/MT9vvrqq9lxxx1rLOvateta1yv0AAAAAFZb37598/zzz+f111/PAw88UB12XHvttTn++OPzwQcfZM8998yIESNyyCGHrNY+69Wrt9yysrKyta7V3VsAAACA1fa5z30u3bp1y4MPPpiHH344V155ZZLklltuyeDBg3PcccdVt33jjTfSsOEnRw+dOnXKlClTctRRR1Uve/7559e6Vld6AAAAAGvksMMOy0033ZTy8vL07t07yUe3oH3iiScyffr0vPrqq7nwwgszderUVFRUfOL+TjjhhPz617/OLbfcUn0XmGeeeWat63SlBwAAANSyyiWL0/2cG2vluPUbrvkwkgMPPDCXXnppDjrooOorOUaMGJELL7wwffv2TfPmzdOzZ8+cddZZGTduXBYsWLDK/e2777657LLLMnbs2Fx55ZXZfffd861vfSvTp0//VM9rGaEHAAAA1LJPEzzU5nGbN2+eZ599tsaynXfeOb/4xS+Wa3viiScmSXr16pVp06ZVL//jH/9Yo92hhx6aQw899FPVszKGtwAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAABrrWLx0touYaXqcm3AZ1NVVVVtl1AYn/RaunsLAABrrbysQfqfc1ttl7FCt488qrZLAKhWVlaWBQsWpGnTprVdSiEsWLAgZWUrvwONKz0AAACgRFq2bJkZM2bkww8/dMXHWqiqqsqHH36YGTNmpGXLlitt50oPAAAAKJHmzZsnSf71r39l8eLFtVzNhq2srCytWrWqfk1XROgBAAAAJdS8efNVflBn3TG8BQAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT0AAACAQhJ6AAAAAIUk9AAAAAAKSegBAAAAFJLQAwAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT0AAACAQhJ6AAAAAIUk9AAAAAAKSegBAAAAFJLQAwAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT0AAACAQhJ6AAAAAIUk9ACANVCxeGltl7BKdb0+AIBSaljbBQDAhqS8rEH6n3NbbZexUrePPKq2SwAAqDNc6QEAAAAUktADAAAAKCShBwAAAFBIQg8AAACgkIQeAAAAQCEJPQAAAIBCEnoAAAAAhST0AAAAAApJ6AEAAAAUktADAAAAKCShBwAAAFBIQg8AAACgkIQeAAAAQCEJPQAAAIBCEnoAAAAAhST0AAAAAApJ6AEAAAAUktADAAAAKCShBwAAAFBIQg8AAACgkIQeAAAAQCEJPQAAAIBCEnoAABukisVLa7uEVarr9QHAZ0HD2i4AAODTKC9rkP7n3FbbZazU7SOPqu0SAFapcsni1G9YVttlrFBdro0Ni9ADAADgM6h+w7I8M3JgbZexQt3PubG2S6AgSjq8ZenSpRk1alR69+6dbt26ZfDgwZkzZ85K2//lL39Jv3790rVr1+y777654YYbUlVVVcKKAQAAgA1VSUOPMWPG5L777suIESMyYcKEzJw5M4MGDVph2zfeeCMnn3xyvvKVr2TSpEkZMmRIfvazn+X2228vZckAAADABqpkoUdFRUXGjx+fM888M3vssUd23HHHjB49OlOmTMmUKVOWa/+nP/0pjRs3zmmnnZZ27dpl//33z1577ZU//elPpSoZAAAA2ICVLPR4+eWXM3/+/PTs2bN6Wdu2bdOmTZtMnjx5ufYtWrTIe++9lwceeCCVlZV55ZVXMnny5Oy0006lKhkAAADYgJUs9Jg5c2aSpFWrVjWWt2zZsnrdf9tvv/3Sr1+/DBkyJDvttFMOPvjg9OjRI6ecckpJ6gUAAAA2bCW7e8uCBQtSv379lJXVvO1QeXl5Fi1atFz7efPm5V//+lcGDhyYAw88MK+88kouvfTSXHPNNRk8ePAaHfv5559fq9oBYJnu3bvXdgmf6JlnnqntEkpCX9Qtdb0/9EXd8lnpjw2hL+qyz8rPCetXyUKPxo0bp7KyMkuWLEnDhv932IqKijRp0mS59ldeeWXq16+fIUOGJEl22GGHLFmyJD/+8Y9zzDHHZLPNNlvtY++0005p1KjR2j8JANgA+CO77tAXdYe+qFv0B6vjk35OFi1a5B/cfKKSDW/ZeuutkySzZ8+usXzWrFnLDXlJkr/97W/Lzd/RpUuXLF68OG+//fb6KxQAAFhvKpcsru0SVqou1wZ8OiW70qNTp05p1qxZnn766Rx66KFJkrfeeiszZsxIjx49lmu/1VZbZdq0aTWWvfrqq6lfv37at29fkpoBAIB1q37DsjwzcmBtl7FC3c+5sbZLANaxkl3pUV5env79+2fkyJF5/PHH88ILL+TMM89Mz54907Vr11RUVGT27NmpqKhIkgwYMCCPPvpoxo4dmzfffDOPPPJILrvssvTv3z8bbbRRqcoGAAAANlAlu9IjSU4//fQsWbIkZ599dpYsWZI999wzw4YNS5JMnTo1AwYMyPjx49OrV6/stddeueaaazJ27NjccMMN2WKLLfLtb387J510UilLBgAAADZQJQ09GjZsmKFDh2bo0KHLrevVq9dyw1n23Xff7LvvvqUqDwAAACiQkg1vAQAAACgloQcAAABQSEIPAAAAoJCEHgAA60HlksW1XcIq1fX6AGBdKOlEpgAAnxX1G5blmZEDa7uMlep+zo21XQIArHeu9AAAAAAKSegBAAAAFJLQAwAAACgkoQcAAABQSEKPDUBdn129rtcHAADAZ5O7tySpWLw05WUNaruMlTL7OwAAAKw5oUeS8rIG6X/ObbVdxkrdPvKo2i4BAAAANjiGtwAAAACFJPQAAKDQ6vr8Y3W9PoANmeEtAAAUmvnRAD67XOkBAAAAFJLQAwAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT0AAACAQhJ6AAAAAIUk9AAAAAAKSegBAAAAFJLQAwAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT0AAACAQhJ6AAAAAIUk9AAAAAAKSegBAAAAFJLQAwAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT0AAACAQhJ6AECBVC5ZXNslrFJdrw8AKJaGtV0AALDu1G9YlmdGDqztMlaq+zk31nYJAMBniCs9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIZU09Fi6dGlGjRqV3r17p1u3bhk8eHDmzJmz0vYzZ87M4MGD061bt+y222758Y9/nAULFpSwYgAAAGBDVdLQY8yYMbnvvvsyYsSITJgwITNnzsygQYNW2LaioiLHHXdc3nvvvdxxxx35yU9+kkcffTRXXHFFKUsGAAAANlANS3WgioqKjB8/PhdccEH22GOPJMno0aOzzz77ZMqUKdlll11qtJ80aVJmz56dO++8M5tsskmS5LTTTsudd95ZqpIBAACADVjJrvR4+eWXM3/+/PTs2bN6Wdu2bdOmTZtMnjx5ufZPPPFEdt999+rAI0n69euXe+65pyT1AgAAABu2koUeM2fOTJK0atWqxvKWLVtWr/tvr7/+etq0aZOrrroqffr0yT777JMRI0Zk0aJFJakXAAAA2LCVbHjLggULUr9+/ZSVldVYXl5evsIg44MPPsg999yTL3/5y7n66qvzzjvv5KKLLsrcuXMzYsSINTr2888/v8r13bt3X6P9sbxnnnmmtksAKAnnjLW3rs4Z+mLtrcvzt/5YO/qi7tAXdYfPGKwLJQs9GjdunMrKyixZsiQNG/7fYSsqKtKkSZPlC2vYMJtssklGjhyZBg0aZOedd86SJUvy/e9/P0OHDs1mm2222sfeaaed0qhRo3XyPFgxb+gArC7njLpDX9Qd+qLu0Bd1xyf1xaJFiz7xH9xQsuEtW2+9dZJk9uzZNZbPmjVruSEvyUfDYLbZZps0aNCgetm2226bJJkxY8Z6rBQAAAAogpKFHp06dUqzZs3y9NNPVy976623MmPGjPTo0WO59rvuumteeumlLF68uHrZK6+8kgYNGqRNmzYlqRkAAADYcJUs9CgvL0///v0zcuTIPP7443nhhRdy5plnpmfPnunatWsqKioye/bsVFRUJEmOOOKILFq0KEOHDs306dPz5z//OVdccUUOPfTQNRraAgAAAHw2lSz0SJLTTz89Bx98cM4+++wMGDAgrVu3ztVXX50kmTp1anr37p2pU6cmSbbYYovcdtttee+99/KNb3wjZ511Vvbbb78MHz68lCUDAAAAG6iSTWSafDQ56dChQzN06NDl1vXq1SvTpk2rsWzbbbfNTTfdVKryAAAAgAIp6ZUeAAAAAKUi9AAAAAAKSegBAAAAFJLQAwAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT2ocyoWL63tElaqLtcGAABATQ1ruwD4uPKyBul/zm21XcYK3T7yqNouAQAAgNXkSg8AAACgkFYZesyZMyfDhw/PO++8U2P5j370owwbNixz585dr8UBAAAAfForDT1mzZqVI444Ir///e/z73//u8a6Dh065JFHHsmRRx4p+AAAAADqpJWGHmPHjs0WW2yR3/zmN9lhhx1qrDv++ONz//33p3Hjxrn22mvXe5EAAAAAa2qlocfjjz+eM844IxtttNEK12+22WY544wz8uijj66v2gAAAAA+tZWGHv/+97/Ttm3bVW687bbbZtasWeu8KAAAAIC1tdLQo2XLlvnnP/+5yo3ffPPNbL755uu8KAAAAIC1tdLQ4ytf+UrGjRuXpUuXrnD90qVLc91112W33XZbb8UBAAAAfForDT1OPPHETJ8+Pd/5znfy2GOP5b333ktlZWXmzp2bRx55JEcffXRefvnlnHzyyaWsFwAAAGC1NFzZii233DL/8z//k7PPPjsnnXRS6tWrV72uqqoqnTt3zi233JJ27dqVpFAAAACANbHS0CP5aKLS++67L88++2xefPHFzJs3L5tttlm6du2a7bbbrlQ1AgAAAKyxVYYey3Tu3DmdO3de37UAAAAArDMrDT3GjRu34g0aNswmm2ySnXfeOZ06dVpvhQEAAACsjZWGHnffffcKl1dVVeX999/PggULsvfee+fqq69OWVnZeisQ6pLKJYtTv2Hd/Xmv6/UBAACU0kpDjz/+8Y+r3PDll1/OmWeembFjx+b73//+Oi8M6qL6DcvyzMiBtV3GSnU/58Z1ur+KxUtTXtZgne5zXarr9a1Ldf25Ll1ckQZl5bVdxgoJAwEAPrtWa06PFenUqVPOPPPMXHHFFUIPKKjysgbpf85ttV3GSt0+8qjaLqFkNoS+qKuB4LoOAwEA2HDUX5uNO3bsmJkzZ66rWgAAAADWmbUKPebPn5+mTZuuq1oAAAAA1pm1Cj3uuOOOdOnSZV3VAgAAALDOrPEtaysrK/PBBx9kypQpeemll3LbbXV3jDkAAADw2bXGt6wtKytL8+bNs+OOO+aSSy7JNttss96KA1iVunxXjrpcGwAAfFZ86lvW/uc//8mvfvWrnH766Zk0adI6Lwzgk9TlWwi7YwgAANS+Nb5l7ZQpU3L33XfnN7/5TRYuXJhOnTqtj7oAAAAA1spqhR7/+c9/MnHixNx999157bXXkiR77LFHBg4cmC996UvrtUAAAACAT2OVocczzzyTu+++O7/97W+zcOHC7LDDDjnzzDNz1VVXZejQodl2221LVScAAADAGllp6HHQQQdl+vTp+eIXv5iTTz45BxxwQDp06JAkueqqq0pVHwAAAMCnUn9lK/7+97+nQ4cO2XvvvbPrrrtWBx4AAAAAG4KVXunx+OOP51e/+lUmTpyYsWPHZvPNN8/++++fr33ta6lXr14pawQAAABYYyu90mOLLbbICSeckEmTJuWuu+7KV7/61UyaNCkDBgzI0qVLc+edd+btt98uZa0AAAAAq22locd/69y5c370ox/liSeeyOjRo/PlL385d9xxR/bdd9+cdtpp67tGAAAAgDW2WresXaasrCwHHHBADjjggMyZMycTJ07Mr371q/VVGwAAAMCntlpXeqzIFltskYEDB2bSpEnrsh4AAACAdeJThx4AAAAAdZnQAwAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT0AAACAQhJ6AAAAAIUk9AAAAAAKSegBAAAAFJLQAwAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT0AAACAQhJ6AAAAAIUk9AAAAAAKSegBAAAAFJLQAwAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT0AAACAQhJ6AAAAAIUk9AAAAAAKSegBAAAAFJLQAwAAACgkoQcAAABQSEIPAAAAoJCEHgAAAEAhCT0AAACAQipp6LF06dKMGjUqvXv3Trdu3TJ48ODMmTNntbY96aSTcswxx6znCgEAAICiKGnoMWbMmNx3330ZMWJEJkyYkJkzZ2bQoEGfuN2dd96ZRx99dP0XCAAAABRGyUKPioqKjB8/PmeeeWb22GOP7Ljjjhk9enSmTJmSKVOmrHS7N954Iz/5yU/SrVu3UpUKAAAAFEDJQo+XX3458+fPT8+ePauXtW3bNm3atMnkyZNXuM3SpUtz7rnnZuDAgdlmm21KVSoAAABQACULPWbOnJkkadWqVY3lLVu2rF73cdddd12S5IQTTli/xQEAAACF07BUB1qwYEHq16+fsrKyGsvLy8uzaNGi5dq/8MIL+fnPf5577rkn9euvXTbz/PPPr3J99+7d12r/JM8888w625f+WDv6ou7QF3WHvqhb1lV/6Iu153ej7tAXdYe+qDvWZV/w2VWy0KNx48aprKzMkiVL0rDh/x22oqIiTZo0qdF20aJFOfvss3P66aenQ4cOa33snXbaKY0aNVrr/bBy3tDrDn1Rd+iLukNf1C36o+7QF3WHvqg79EXd8Ul9sWjRok/8BzeULPTYeuutkySzZ8+u/j5JZs2atdyQl7/97W+ZPn16rrzyylx55ZVJPgpHKisr061btzz44INp3bp1qUoHAAAANkAlCz06deqUZs2a5emnn86hhx6aJHnrrbcyY8aM9OjRo0bbzp0753e/+12NZaNHj86//vWvXHnllWnZsmWpygYAAAA2UCULPcrLy9O/f/+MHDkym222WTbffPMMHz48PXv2TNeuXVNRUZH3338/m2yySRo3brzcsJaNNtpohcsBAAAAVqRkd29JktNPPz0HH3xwzj777AwYMCCtW7fO1VdfnSSZOnVqevfunalTp5ayJAAAAKCgSnalR5I0bNgwQ4cOzdChQ5db16tXr0ybNm2l215yySXrszQAAACgYEp6pQcAAABAqQg9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEglDT2WLl2aUaNGpXfv3unWrVsGDx6cOXPmrLT9Qw89lEMPPTRdu3bNV7/61Vx//fVZunRpCSsGAAAANlQlDT3GjBmT++67LyNGjMiECRMyc+bMDBo0aIVtH3vssQwZMiTf/OY3c//99+ess87KDTfckHHjxpWyZAAAAGADVbLQo6KiIuPHj8+ZZ56ZPfbYIzvuuGNGjx6dKVOmZMqUKcu1v/POO7Pffvvl6KOPTvv27bP//vvn2GOPzb333luqkgEAAIANWMNSHejll1/O/Pnz07Nnz+plbdu2TZs2bTJ58uTssssuNdp/73vfS9OmTWssq1+/fubNm1eSegEAAIANW8lCj5kzZyZJWrVqVWN5y5Ytq9f9t86dO9d4/MEHH+SOO+7InnvuucbHfv7551e5vnv37mu8T2p65pln1tm+9Mfa0Rd1h76oO/RF3bKu+kNfrD2/G3WHvqg79EXdsS77gs+ukoUeCxYsSP369VNWVlZjeXl5eRYtWvSJ255yyilZtGhRzjrrrDU+9k477ZRGjRqt8XasPm/odYe+qDv0Rd2hL+oW/VF36Iu6Q1/UHfqi7vikvli0aNEn/oMbSjanR+PGjVNZWZklS5bUWF5RUZEmTZqsdLu5c+fmuOOOy4svvpgbbrghbdq0Wd+lAgAAAAVQstBj6623TpLMnj27xvJZs2YtN+RlmbfeeitHHnlk3nrrrUyYMGG5IS8AAAAAK1Oy0KNTp05p1qxZnn766eplb731VmbMmJEePXos1/7f//53BgwYkMrKytxxxx3p1KlTqUoFAAAACqBkc3qUl5enf//+GTlyZDbbbLNsvvnmGT58eHr27JmuXbumoqIi77//fjbZZJOUl5dn+PDheffdd3PLLbekcePG1VeI1KtXL1tssUWpygYAAAA2UCULPZLk9NNPz5IlS3L22WdnyZIl2XPPPTNs2LAkydSpUzNgwICMHz8+Xbp0ye9///tUVlbmm9/8Zo19NGjQIC+++GIpywYAAAA2QCUNPRo2bJihQ4dm6NChy63r1atXpk2bVv34pZdeKmVpAAAAQMGUbE4PAAAAgFISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACFJPQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQShp6LF26NKNGjUrv3r3TrVu3DB48OHPmzFlp++eeey5HHHFEunTpkv322y8TJ04sXbEAAADABq2koceYMWNy3333ZcSIEZkwYUJmzpyZQYMGrbDt3LlzM3DgwOy444659957c8wxx+T888/PE088UcqSAQAAgA1Uw1IdqKKiIuPHj88FF1yQPfbYI0kyevTo7LPPPpkyZUp22WWXGu1/8YtfZKONNsr555+f+vXrZ5tttsmLL76Ym2++Ob179y5V2QAAAMAGqmShx8svv5z58+enZ8+e1cvatm2bNm3aZPLkycuFHpMnT06PHj1Sv/7/XYzSs2fPDB8+PJWVlTWWr0xVVVWSjwKXT9K8adnqPpWSW7RoUdJ449ouY6UWLVq0zvdZV/tDX9Qtdbk/9EXdoS/qlnXdH/ri0/ss/W7oi7qlLveHvqg7Vqcvln3OW/a5D1akXlWJfkJ+97vfZdCgQXn++edTVvZ/v/hHHHFEdthhhwwbNqxG+4MPPjh9+vTJGWecUb3sySefzPHHH5+//OUvadGixSce8z//+U9eeeWVdfckAAAAqFO23377bLxx3QxvqH0lu9JjwYIFqV+/fo3AI0nKy8tXmOItXLgw5eXly7VNVu/KjSRp1qxZtt9++5SVlaVevXqfsnIAAADqmqqqqixevDjNmjWr7VKow0oWejRu3DiVlZVZsmRJGjb8v8NWVFSkSZMmK2z/8XBj2eMVtV+R+vXrS/wAAAAKqnHjxrVdAnVcye7esvXWWydJZs+eXWP5rFmz0qpVq+Xab7XVVits27RpU0EGAAAA8IlKFnp06tQpzZo1y9NPP1297K233sqMGTPSo0eP5dp37949kydPrjEpzVNPPZVddtlltSYxBQAAAD7bSpYelJeXp3///hk5cmQef/zxvPDCCznzzDPTs2fPdO3aNRUVFZk9e3b1EJZ+/fpl7ty5+dGPfpTp06fn1ltvzQMPPJCBAweWqmQAAABgA1ayu7ckyZIlS3LllVfmvvvuy5IlS7Lnnntm2LBhadGiRZ566qkMGDAg48ePT69evZIk/+///b9cfPHFmTZtWlq3bp3Bgwfn61//eqnKBQAAADZgJQ09AAAAAErF5BgAAABAIQk9AAAAgEISegAAAACF1LC2C/is6NOnT/r165dTTjmletnSpUtz1lln5ZFHHsm1116bCy64IA0aNMj999+fJk2a1Nj+mGOOSfv27XPJJZckSTp27Jhu3brl9ttvX+4Wvis6FqtnVa/d0KFDc99999VYVlZWls033zx77713zjnnnDRt2rRUpRbKxIkTM2HChLz22mupV69eOnbsmAEDBuTAAw+sblNZWZm77rorEydOzN///vcsWrQoHTp0yNe//vUcd9xxadSoUZJUT4q8TL169dKkSZNst912+c53vmMy5DXUp0+f1Xpf6tixY411TZo0yRe+8IUMGjQoe++9dylLLrQ+ffpkxowZ1Y/LysrSqlWr7Lfffjn11FOz0UYbLdfm4/r27ZvLL7+8FOUW2ope58aNG6d169b59re/nWOPPXal7ZYZN26c3491ZHXOER8/PyTJxhtvnG7dumXo0KHZZpttaqn6uuuf//xnDj300Bx88MG58MILa6x75JFHcsopp+Smm27K7rvv/qnP08mq++H111/PDTfckCeffDJz585Nq1atsv/+++fEE0/MxhtvvH5fgA3Iqt5rtttuu2y22WZ5+umnV7p9z549c+uttyZJKioqcuutt2bSpEl544030qRJk3Tu3Dmnnnpqdt555/VSP6xPQo9aUllZmXPPPTePPvpoxo0bl9122y3JRyeX0aNH5/zzz//EfUydOjXjx4+v/sOK9W/XXXfNVVddVf14wYIF+fOf/5yLL744VVVVGT58eO0Vt4G66667MmLEiFxwwQXp3r17Fi9enIcffjhnnnlmFi1alL59+2bJkiU56aST8uKLL+bUU0/NbrvtlkaNGmXq1Km56qqr8te//jU///nPU69ever93nfffdlyyy1TWVmZd999Nw8++GDOOuusvPfeeznqqKNq8RlveFb3fWnYsGHZb7/9UlVVlQ8++CAPPfRQTjvttPzyl79Mp06dSlRt8X33u9/Nd77znSQfvQc9//zzufzyy6vPCffcc0+WLl2aJHnooYcyYsSIPPbYY9XbN27cuFbqLqL/7oskee+993LnnXfmsssuS8uWLauD24+3W2aTTTYpWa1FtrrniGU+fn645pprcsIJJ+S3v/1t9QdzPtK+ffsMHTo0w4YNyz777JO99torSfL2229n6NChGThwYHbfffd1cp5eUT889dRTOfnkk9O7d+9ceeWVadWqVV577bWMGDEiTz75ZG699dY0a9asVl6bumhl7zUNGzZMvXr1snjx4iQffYYYNGhQdR8kH4XoyUfnlQEDBuTdd9/N4MGD06VLl8yfPz/jx4/PUUcdleuvvz5f+tKXSvekYB0QetSCqqqqnH/++fnDH/6Q6667rvoWvUnSrl27TJgwIQcccEB22WWXVe6nXbt2ueqqq7LPPvukXbt267ts8tEJYdnJYZn27dvn2Wefza9//Wuhx6dw11135Vvf+la+8Y1vVC/bdttt849//CPjx49P3759c/PNN+epp57KL3/5yxpXFLRt2zZdunTJAQcckMceeyxf+cpXqte1aNGiuq9atWqVTp06ZcGCBbnyyitzwAEHpEWLFiV7jhu61X1f2mijjapf85YtW+a0007LpEmTMmnSJKHHOtS0adMa70Pt27dPhw4dcvjhh+eXv/xljjzyyOp1y/4L+vH3LdaNj/fFlltumR/+8Id5/PHH89BDD1WHHh9vx7q1uueIZVerffz8MGzYsOy5557561//Wv2hnv/z7W9/O4888kjOP//83H///WnevHnOOOOMdOjQId///veTrH4frOo8/fF+WLRoUYYMGZK99tqrxj+c2rVrl44dO+ZrX/tabrvttpx44okleR02BKv7XrMscP3vPljmqquuyuuvv54HHnggrVq1ql5++eWX59///ncuuuiiPPDAAzUCLKjrzOlRYlVVVRk2bFh+85vf5Prrr68ReCQfXXbcrVu3nH/++Vm0aNEq93XiiSemZcuWOf/88+POw7WrvLw8DRvKED+N+vXrZ8qUKfnPf/5TY/m5556bMWPGpKqqKrfffnsOO+yw5YZQJB994HvooYdW6w/V73znO/nwww/z6KOPrqvyPxPW5H3p45o2beoPoxLYcccd07179zz00EO1XQr5KCB3TiiNdXGOWDY01XvVyl188cVZunRpLr744owbNy6vvvpqRo0alYYNG66z8/TH++GPf/xjZs2atcIhx61bt84tt9ySww8/fB08O5apqKjIvffem379+tUIPJYZNmxYRo0a5XeFDY7Qo8QuvPDC3H333fn+97+fHj16LLe+Xr16ufTSS/Ovf/0rY8aMWeW+GjVqlEsuuSRPP/107rzzzvVVMquwdOnSPPbYY/nVr36Vgw8+uLbL2SCdcMIJefbZZ7Pnnnvm5JNPzk033ZSXXnopLVq0SNu2bfPWW2/l7bffXuWllB06dFitE3C7du3SpEmTvPLKK+vyKRTemrwvLbNkyZI88MADmT59eg499ND1XCFJsv322/vZrmULFizIjTfemOnTpzsnlMjaniM+/PDDXH311Wnfvr1L9ldhiy22yEUXXZQHH3ww1157bS688MLqq4zXxXl6Rf3wwgsvpGnTptl+++1XuM0uu+ySzTfffC2eFR/35ptvZt68eenSpcsK17dr186Vm2yQ/BuihG6//fZ8+OGH6dy5c2688cYccsghK7zE/nOf+1wGDRqU0aNHZ//9989OO+200n326NEjRx55ZK644op85StfydZbb70+n8Jn3tNPP51u3bpVP164cGG23nrrHH/88Tn55JNrsbIN1wEHHJBWrVrllltuyZNPPplHHnkkSbLDDjtk5MiR+eCDD5Ikm222WY3tDjnkkLz55pvVj1c0ydqKNG/evHqfrL7VeV+64IIL8uMf/zhJsmjRoixdujRHH320yQFLxM926Y0dOzY33HBDko+uOFi0aFE6duyY0aNHZ5999llhu2UGDhyYU089taT1FtGcOXOSrN45YtlE1vvvv3/q1auXqqqqLFy4MEkyevTolJeXl6jqDVOPHj2yySabZOHChTX+FlqTPvjv8/Qn9cO8efNMVLqGVvRek3w0Gf+3v/3tT9x+3rx5ST46n0CRCD1K6MMPP8xNN92U1q1b5+CDD84PfvCDjBs3boVtjzvuuPz2t7/Neeedl3vvvXeV+x0yZEgee+yx/PCHP8yNN964Pkrn/9e5c+eMGDEiVVVVeemll3LxxRenZ8+eOfnkk6sngGLN7bLLLtlll12ydOnSvPDCC/njH/+YCRMm5Lvf/W715HPvv/9+jW3GjRtXPSHXueeem4qKitU61gcffOCPqE/pk96XzjjjjOoPegsXLqyeYHPp0qXVYQjrz/z58/1sl9hRRx2V/v37Z+nSpfnDH/6QsWPH5hvf+MZyd4la1u6/mcR03dh0002TrNk54sYbb8yWW26Zqqqq/Oc//8kjjzySIUOGpKqqyh2+VuGHP/xhNt100yxZsiTnnntubrnlltSvX/9T9UHyyf2w2WabZd68eamqqjKcYjWt6L0myWrPY7YsuHrvvffWZVlQ64QeJXTcccdVJ+PDhg3LWWedlQkTJuToo49erm2DBg1y6aWXpm/fvisNRpZp1qxZLrroohx//PGfGJCwdho3bpwOHTok+eg/31tttVWOPvrolJeXr9ZVBtT09ttv57rrrsupp56aLbfcMg0aNEjnzp3TuXPn7LrrrjnhhBMyb968bLHFFpk8eXKNW9i2bt26+vvVvRvFG2+8kfnz52fHHXdc58/ls+CT3pc233zz6t+P5KNba8+aNStXX311hgwZko022qiU5X7mvPDCC362S2yTTTap/pn/whe+kPr16+eSSy5JixYtctBBB62wHetW+/bt1/gc0bZt22y11VbVj3feeedMnTo1N998s9BjJe688878/ve/zy233JKlS5fmuOOOy80335yBAwd+qj5IPrkfunbtmnHjxmXatGkrHFIxYsSING3aNIMGDVrHz3bDtbbvNe3bt8/mm2+ev/3tbzX6cpmnnnoqP//5z3PhhRemZcuWa1MqlJQ5PUqoQYMG1d8fdNBBOfDAAzNy5MiVjsHebrvt8r3vfS/XXXdd/vnPf65y33vssUcOP/zwXH755S5vLqFu3bpl4MCBueuuu/L444/XdjkbnEaNGuWee+7JAw88sNy65s2bp169etlyyy1z1FFH5d5778306dOXa1dRUZG5c+eu1vFuv/32bLTRRjVmj2fNrMn7UpLqSZZNtrx+vfzyy5k6dWqND9qU3vHHH5/u3btn+PDhmT17dm2X85nQoEGDdXKOqKqq8j61EtOnT89ll12W448/Pj179sxuu+2Wo48+OldddVVefvnlddYHSc1+2GOPPdK6detce+21y7V74403cscdd9T425q1V79+/fTt2ze//OUv884779RYV1VVleuvvz7/+Mc/3I2KDY7Qoxb96Ec/SvPmzXPWWWet9I4IJ510UrbddtvMnDnzE/d33nnnpXHjxstdXsiaeeONN/L444/X+Prb3/620vannnpqPve5z+XHP/5xPvzwwxJWuuFr0aJFTjjhhIwaNSpjxozJtGnT8sYbb+T3v/99zjvvvPTt2zetW7fOiSeemN122y1HHnlkfv7zn+fVV1/Nm2++mUmTJuXwww/P3//+93Tv3r3GvufOnZvZs2fnnXfeycsvv5zLLrss48ePz9ChQ11xsJZW9r70wQcfZPbs2dWv+8MPP5xbbrklffr0MexiHfrwww+rX+c333wzEydOzHe/+9306NEjhxxySG2X95lWr169XHTRRVm4cGEuvvji2i7nM2NNzxHLzg+zZ8/OW2+9lZtuuil//etf/f6swKJFi3LGGWfk85//fPXtaZOPhla3a9cuZ599dioqKtbqPL2yfigvL8/FF1+cRx55JIMHD84zzzyTN998Mw899FCOO+64bLfddjnuuONK+nrUdf99fvj41+qGeqecckratm2b/v3754EHHsibb76ZqVOnZvDgwfnf//3fXHLJJYYbscExvKUWbbrpprnkkkty4oknZsSIESts07Bhw1x66aX55je/+Yn723jjjTN8+HATaq6liRMnZuLEiTWW7bLLLiu9XLC8vDwXXXRRBgwYkKuvvjrnnXdeCaosjjPOOCMdOnTI3Xffnf/5n//JokWL0r59+/Tt2zfHHntsko9+D8aOHZtf/epXuffeezNu3Lh8+OGHad26dXr37p0xY8bkc5/7XI399u3bN8lHH0I233zzdOzYMePGjVutW9uyait7X7rwwgurh3k1bNgwrVq1yje+8Y0V3m6QT++GG26onqiuWbNmadOmTfr3759jjz3Wfz3rgG222SYnnXRSxowZkz/84Q+1Xc5nwuqeI5566qkk/3d+SD46h3/+85/P+eefv8Lhxp91I0eOzOuvv5577723xkSvjRs3zsiRI3PEEUdk1KhROe+88z71eTpZeT/sscceueOOO3Ldddfl9NNPz/vvv5+tt946Bx98cL773e+mSZMm6/012JD89/nh4/7yl7+s1twezZo1y4QJE3LDDTfkmmuuydtvv52NN944Xbp0yV133ZUvfvGL67psWO/qVbmWDwAAACggw1sAAACAQhJ6AAAAAIUk9AAAAAAKSegBAAAAFJLQAwAAACgkoQcAAABQSEIPAFiPKioqctNNN+Wwww5Lt27dsvvuu+fkk0/Oc889lyR566230rFjx0yePHm91zJmzJh89atfrX78pz/9KX369MnOO++c8ePHp0+fPhk7dux6rwMAoFTqVVVVVdV2EQBQRAsWLMiAAQPy7rvvZvDgwenSpUvmz5+f8ePH56GHHsr111+ftm3bZp999sltt92WXXfddb3WM3/+/CxatCgtWrRIkhx++OHZdNNNM3z48Gy66aapqKhI48aN07Rp0/VaBwBAqTSs7QIAoKiuuuqqvP7663nggQfSqlWr6uWXX355/v3vf+eiiy7KuHHjSlZPs2bN0qxZs+rH//nPf7LXXnulbdu2JasBAKCUDG8BgPWgoqIi9957b/r161cj8Fhm2LBhGTVqVOrVq1dj+XvvvZfzzjsvvXv3zo477pjevXtnxIgRqaysTJLMmTMnp512Wnr16pWuXbvm2GOPzUsvvVS9/b333psDDjggO+20U/bee+/89Kc/rd72v4e3dOzYMW+88UZ+9rOfpWPHjkmy3PCWhx9+OIccckh23nnn7L///rnpppuq97VsWM64ceOy22675YADDkhFRcU6fAUBANaeKz0AYD148803M2/evHTp0mWF69u1a5fko/Dgv5177rl59913c+2112bTTTfN448/nosuuijdu3fPvvvum+HDh2fJkiW5/fbbU69evYwaNSqDBg3Kww8/nJdffjnDhg3L6NGjs9NOO+WFF17IkCFD0r59+xx22GE1jvPEE0/k29/+dr72ta/l+OOPX66+xx57LEOGDMkFF1yQnj175tVXX82FF16YBQsW5LTTTqtu9+CDD2bChAlZuHBhysvL1/JVAwBYt4QeALAezJs3L0nSvHnzNdpuzz33TK9evbLddtslSY466qjceOONmTZtWvbdd9+88cYb6dixY9q2bZtGjRrlwgsvzGuvvZbKysq8+eabqVevXlq3bl399fOf/zxbbbXVcsfZcsst06BBgzRt2jRbbrnlcuvHjRuXI488Mv369UuStG/fPvPnz88Pf/jDnHLKKdXtjjrqqGyzzTZr9BwBAEpF6AEA68Fmm22W5KPhKmviyCOPzB/+8If84he/yOuvv55p06Zl5syZ1cNKTjnllJx77rn53e9+lx49euTLX/5yDjvssNSvXz977rlnunTpksMPPzwdOnRI7969c+CBB6Z169ZrXP9LL72U5557LnfeeWf1ssrKyixcuDAzZsyoHpaz7IoVAIC6yJweALAetG/fPptvvnn+9re/rXD9U089lZNPPjmzZ8+uXlZVVZUTTzwxl19+eZo0aZJDDz00EyZMSJs2barb7L///vnTn/6Uiy++OFtuuWXGjh2bww47LHPmzEnjxo0zYcKE3HPPPTn00EPz4osv5uijj84NN9ywxvWXlZXl5JNPzsSJE6u/7r///vzud7+rMUdJo0aN1njfAAClIvQAgPWgfv366du3b375y1/mnXfeqbGuqqoq119/ff7xj39kiy22qF7+2muv5YknnsiYMWNyxhln5Otf/3o222yzzJ49O1VVVVmyZElGjBiRGTNm5OCDD85ll12WBx98MDNmzMjTTz+dJ598Mj/72c+y884759RTT82dd96ZI444Ivfdd98a17/tttvm9ddfT4cOHaq/XnnllfzkJz9Z69cGAKBUDG8BgPXklFNOyZNPPpn+/fvnjDPOSJcuXTJnzpzcfPPN+d///d/cfPPNNe7e0rx58zRs2DC//vWvs8kmm2T27Nn5yU9+koqKilRUVKRhw4Z54YUXMnny5FxwwQVp0aJFJk2alLKysuy4445555138rOf/Swbb7xx9t5778yZMydPPfVUunbtusa1f+9738tJJ52U7bffPvvtt19ef/31DBs2LHvttZcJSwGADYbQAwDWk2bNmmXChAm54YYbcs011+Ttt9/OxhtvnC5duuSuu+7KF7/4xRp3b2nVqlUuvfTSjBkzJrfccktatWqVAw44IK1atcpzzz2XJBk1alQuvfTSnHTSSZk/f3622267/OxnP6u+GuPSSy/NjTfemCuvvDIbbbRR9t1335xzzjlrXPuXv/zljBw5Mtdff31++tOfpkWLFjnssMNyxhlnrLPXBwBgfatXVVVVVdtFAAAAAKxr5vQAAAAACknoAQAAABSS0AMAAAAoJKEHAAAAUEhCDwAAAKCQhB4AAABAIQk9AAAAgEISegAAAACF9P8BzeyUf32Xh+QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(16, 8))\n",
    "ax = sns.barplot(x = 'classifier', y = 'auc', hue = 'data_set', data = df_results)\n",
    "ax.set_xlabel('Classifier', fontsize = 15)\n",
    "ax.set_ylabel('AUC', fontsize = 15)\n",
    "ax.tick_params(labelsize = 15)\n",
    "\n",
    "#Separate legend from graph\n",
    "plt.legend(bbox_to_anchor = (1.05, 1), loc = 2, borderaxespad = 0., fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:61317</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:61317' processes=4 threads=8, memory=17.18 GB>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:33:12] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:12] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:12] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:13] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:15] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:33:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:16] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:19] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:33:20] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597d5e9f44ed49579f9ddec4d052e741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Optimization Progress'), FloatProgress(value=0.0, max=10100.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\r\n",
      "\r\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A pipeline has not yet been optimized. Please call fit() first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-26e0a3588b53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m }\n\u001b[1;32m     22\u001b[0m \u001b[0mpipeline_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTPOTClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiodic_checkpoint_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tpot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_dask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpot_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mpipeline_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mpipeline_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tpot_exported_pipeline.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    861\u001b[0m                     \u001b[0;31m# raise the exception if it's our last attempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattempts\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, features, target, sample_weight, groups)\u001b[0m\n\u001b[1;32m    852\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_top_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_summary_of_best_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0;31m# Delete the temporary cache before exiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/tpot/base.py\u001b[0m in \u001b[0;36m_update_top_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0;31m# If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m             \u001b[0;31m# need raise RuntimeError because no pipeline has been optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    962\u001b[0m                 \u001b[0;34m\"A pipeline has not yet been optimized. Please call fit() first.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A pipeline has not yet been optimized. Please call fit() first."
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "import numpy as np\n",
    "tpot_config = {\n",
    "    'xgboost.XGBClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': range(1, 11),\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'nthread': [1]\n",
    "    },\n",
    "    'sklearn.ensemble.ExtraTreesClassifier': {\n",
    "        'n_estimators': [100],\n",
    "        'criterion': [\"gini\", \"entropy\"],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "}\n",
    "pipeline_optimizer = TPOTClassifier(verbosity=2, periodic_checkpoint_folder='tpot', generations=100, population_size=100, scoring='roc_auc', use_dask=True, config_dict=tpot_config)\n",
    "pipeline_optimizer.fit(X_train_tf, y_train)\n",
    "print(pipeline_optimizer.score(X_valid, y_valid))\n",
    "pipeline_optimizer.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/robert/eeg_data/edf/train/03_tcp_ar_a/104/00010418/s017_2014_05_31/00010418_s017_t001.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 153855  =      0.000 ...   600.996 secs...\n"
     ]
    }
   ],
   "source": [
    "# Inference on specific example\n",
    "example_file = '/Users/robert/eeg_data/edf/train/03_tcp_ar_a/104/00010418/s017_2014_05_31/00010418_s017_t001.edf'\n",
    "\n",
    "load_example = mne.io.read_raw_edf(example_file, preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw: mne.io.edf.edf.RawEDF):\n",
    "    samp_freq = raw.info['sfreq']\n",
    "    nchannels = raw.info['nchan']\n",
    "    nyquist_freq = int(samp_freq / 2)\n",
    "    freqs = range(60, nyquist_freq, 60)\n",
    "    # Notch filter\n",
    "    notch_filtered = mne.filter.notch_filter(raw._data, samp_freq, freqs)\n",
    "    # TODO: Bandpass filter\n",
    "    # TODO: Referencing op\n",
    "    # Downsampling\n",
    "    notch_filtered = mne.io.RawArray(notch_filtered, raw.info)\n",
    "    downsampled = notch_filtered.resample(samp_freq / 2)._data\n",
    "    # Reshape to have 64 channels, to fit model\n",
    "    # TODO: Better strat than this later\n",
    "    num_samples = np.shape(downsampled)[1]\n",
    "    if (nchannels > 64):\n",
    "        # pick only the first 64\n",
    "        downsampled = downsampled[0:64]\n",
    "    else:\n",
    "        # Repeat the channels till we get 64\n",
    "        right_nchannels = np.zeros((64, num_samples))\n",
    "        num_reps = math.floor(64 / nchannels)\n",
    "        for i in range(num_reps):\n",
    "            start_ind = i * nchannels\n",
    "            end_ind = (i + 1) * nchannels\n",
    "            # Downsampled end index, for times when 64 isn't\n",
    "            # a multiple of nchannels\n",
    "            dend_ind = nchannels\n",
    "            if (end_ind > 64):\n",
    "                overflow = end_ind - 64\n",
    "                end_ind = 64\n",
    "                dend_ind = nchannels - overflow\n",
    "\n",
    "            right_nchannels[i * nchannels: (i+1) * nchannels] = downsampled[0:dend_ind]\n",
    "        downsampled = right_nchannels\n",
    "\n",
    "    # Needs another reshaping to be 64 * (multiple of 128) * 1\n",
    "    return downsampled.reshape(64, num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n",
      "Creating RawArray with float64 data, n_channels=31, n_times=153856\n",
      "    Range : 0 ... 153855 =      0.000 ...   600.996 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "preprocessed_example = preprocess(load_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 76928, 1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edf_length(raw: mne.io.edf.edf.RawEDF):\n",
    "    return np.shape(raw._data)[1]/raw.info['sfreq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf_length = get_edf_length(load_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/robert/eeg_data/edf/train/02_tcp_le/024/00002448/s001_2006_01_26/00002448_s001_t004.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 234999  =      0.000 ...   939.996 secs...\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 1651 samples (6.604 sec)\n",
      "\n",
      "Creating RawArray with float64 data, n_channels=33, n_times=235000\n",
      "    Range : 0 ... 234999 =      0.000 ...   939.996 secs\n",
      "Ready.\n",
      "bin: 0  result: 0.1\n",
      "bin: 1  result: 0.0\n",
      "bin: 2  result: 0.0\n",
      "bin: 3  result: 0.0\n",
      "bin: 4  result: 0.0\n",
      "bin: 5  result: 0.0\n",
      "bin: 6  result: 0.0\n",
      "bin: 7  result: 0.2\n",
      "bin: 8  result: 0.2\n",
      "bin: 9  result: 0.0\n",
      "bin: 10  result: 0.0\n",
      "bin: 11  result: 0.0\n",
      "bin: 12  result: 0.0\n",
      "bin: 13  result: 0.0\n",
      "bin: 14  result: 0.1\n",
      "bin: 15  result: 0.3\n",
      "bin: 16  result: 0.2\n",
      "bin: 17  result: 0.0\n",
      "bin: 18  result: 0.1\n",
      "bin: 19  result: 0.1\n",
      "bin: 20  result: 0.2\n",
      "bin: 21  result: 0.2\n",
      "bin: 22  result: 0.0\n",
      "bin: 23  result: 0.0\n",
      "bin: 24  result: 0.0\n",
      "bin: 25  result: 0.0\n",
      "bin: 26  result: 0.1\n",
      "bin: 27  result: 0.2\n",
      "bin: 28  result: 0.2\n",
      "bin: 29  result: 0.2\n",
      "bin: 30  result: 0.1\n",
      "bin: 31  result: 0.0\n",
      "bin: 32  result: 0.1\n",
      "bin: 33  result: 0.1\n",
      "bin: 34  result: 0.2\n",
      "bin: 35  result: 0.2\n",
      "bin: 36  result: 0.0\n",
      "bin: 37  result: 0.2\n",
      "bin: 38  result: 0.3\n",
      "bin: 39  result: 0.1\n",
      "bin: 40  result: 0.1\n",
      "bin: 41  result: 0.3\n",
      "bin: 42  result: 0.2\n",
      "bin: 43  result: 0.0\n",
      "bin: 44  result: 0.1\n",
      "bin: 45  result: 0.2\n",
      "bin: 46  result: 0.1\n",
      "bin: 47  result: 0.0\n",
      "bin: 48  result: 0.3\n",
      "bin: 49  result: 0.5\n",
      "bin: 50  result: 0.4\n",
      "bin: 51  result: 0.3\n",
      "bin: 52  result: 0.1\n",
      "bin: 53  result: 0.1\n",
      "bin: 54  result: 0.3\n",
      "bin: 55  result: 0.2\n",
      "bin: 56  result: 0.2\n",
      "bin: 57  result: 0.2\n",
      "bin: 58  result: 0.1\n",
      "bin: 59  result: 0.2\n",
      "bin: 60  result: 0.1\n",
      "bin: 61  result: 0.0\n",
      "bin: 62  result: 0.3\n",
      "bin: 63  result: 0.4\n",
      "bin: 64  result: 0.3\n",
      "bin: 65  result: 0.3\n",
      "bin: 66  result: 0.2\n",
      "bin: 67  result: 0.1\n",
      "bin: 68  result: 0.0\n",
      "bin: 69  result: 0.0\n",
      "bin: 70  result: 0.0\n",
      "bin: 71  result: 0.0\n",
      "bin: 72  result: 0.0\n",
      "bin: 73  result: 0.1\n",
      "bin: 74  result: 0.3\n",
      "bin: 75  result: 0.2\n",
      "bin: 76  result: 0.0\n",
      "bin: 77  result: 0.3\n",
      "bin: 78  result: 0.3\n",
      "bin: 79  result: 0.2\n",
      "bin: 80  result: 0.3\n",
      "bin: 81  result: 0.4\n",
      "bin: 82  result: 0.3\n",
      "bin: 83  result: 0.0\n",
      "bin: 84  result: 0.0\n",
      "bin: 85  result: 0.0\n",
      "bin: 86  result: 0.0\n",
      "bin: 87  result: 0.0\n",
      "bin: 88  result: 0.1\n",
      "bin: 89  result: 0.1\n",
      "bin: 90  result: 0.0\n",
      "bin: 91  result: 0.0\n",
      "bin: 92  result: 0.1\n",
      "bin: 93  result: 0.1\n",
      "bin: 94  result: 0.1\n",
      "bin: 95  result: 0.2\n",
      "bin: 96  result: 0.1\n",
      "bin: 97  result: 0.0\n",
      "bin: 98  result: 0.0\n",
      "bin: 99  result: 0.0\n",
      "bin: 100  result: 0.0\n",
      "bin: 101  result: 0.1\n",
      "bin: 102  result: 0.1\n",
      "bin: 103  result: 0.0\n",
      "bin: 104  result: 0.0\n",
      "bin: 105  result: 0.0\n",
      "bin: 106  result: 0.0\n",
      "bin: 107  result: 0.0\n",
      "bin: 108  result: 0.0\n",
      "bin: 109  result: 0.3\n",
      "bin: 110  result: 0.3\n",
      "bin: 111  result: 0.0\n",
      "bin: 112  result: 0.0\n",
      "bin: 113  result: 0.2\n",
      "bin: 114  result: 0.2\n",
      "bin: 115  result: 0.1\n",
      "bin: 116  result: 0.3\n",
      "bin: 117  result: 0.4\n",
      "bin: 118  result: 0.2\n",
      "bin: 119  result: 0.1\n",
      "bin: 120  result: 0.5\n",
      "bin: 121  result: 0.7\n",
      "bin: 122  result: 0.3\n",
      "bin: 123  result: 0.2\n",
      "bin: 124  result: 0.6\n",
      "bin: 125  result: 0.8\n",
      "bin: 126  result: 0.4\n",
      "bin: 127  result: 0.2\n",
      "bin: 128  result: 0.4\n",
      "bin: 129  result: 0.5\n",
      "bin: 130  result: 0.3\n",
      "bin: 131  result: 0.3\n",
      "bin: 132  result: 0.7\n",
      "bin: 133  result: 0.7\n",
      "bin: 134  result: 0.3\n",
      "bin: 135  result: 0.3\n",
      "bin: 136  result: 0.8\n",
      "bin: 137  result: 0.7\n",
      "bin: 138  result: 0.2\n",
      "bin: 139  result: 0.3\n",
      "bin: 140  result: 0.8\n",
      "bin: 141  result: 0.7\n",
      "bin: 142  result: 0.2\n",
      "bin: 143  result: 0.3\n",
      "bin: 144  result: 0.8\n",
      "bin: 145  result: 0.6\n",
      "bin: 146  result: 0.1\n",
      "bin: 147  result: 0.5\n",
      "bin: 148  result: 1.0\n",
      "bin: 149  result: 0.6\n",
      "bin: 150  result: 0.2\n",
      "bin: 151  result: 0.2\n",
      "bin: 152  result: 0.1\n",
      "bin: 153  result: 0.0\n",
      "bin: 154  result: 0.0\n",
      "bin: 155  result: 0.0\n",
      "bin: 156  result: 0.0\n",
      "bin: 157  result: 0.0\n",
      "bin: 158  result: 0.0\n",
      "bin: 159  result: 0.0\n",
      "bin: 160  result: 0.0\n",
      "bin: 161  result: 0.0\n",
      "bin: 162  result: 0.0\n",
      "bin: 163  result: 0.0\n",
      "bin: 164  result: 0.0\n",
      "bin: 165  result: 0.0\n",
      "bin: 166  result: 0.0\n",
      "bin: 167  result: 0.0\n",
      "bin: 168  result: 0.0\n",
      "bin: 169  result: 0.0\n",
      "bin: 170  result: 0.0\n",
      "bin: 171  result: 0.0\n",
      "bin: 172  result: 0.0\n",
      "bin: 173  result: 0.0\n",
      "bin: 174  result: 0.1\n",
      "bin: 175  result: 0.1\n",
      "bin: 176  result: 0.0\n",
      "bin: 177  result: 0.0\n",
      "bin: 178  result: 0.0\n",
      "bin: 179  result: 0.2\n",
      "bin: 180  result: 0.2\n",
      "bin: 181  result: 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (64,124,1) into shape (64,128,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-8ffa5ddeb604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mbatch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbatch_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mbatched_bins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_bin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mbins_squeezed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_bins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (64,124,1) into shape (64,128,1)"
     ]
    }
   ],
   "source": [
    "# Inference on specific example\n",
    "# example_file = '/Users/robert/eeg_data/edf/train/03_tcp_ar_a/104/00010418/s017_2014_05_31/00010418_s017_t001.edf'\n",
    "example_file = '/Users/robert/eeg_data/edf/train/02_tcp_le/024/00002448/s001_2006_01_26/00002448_s001_t004.edf'\n",
    "# example_file = '/Users/robert/eeg_data/edf/train/02_tcp_le/051/00005101/s002_2008_10_22/00005101_s002_t000.edf'\n",
    "load_example = mne.io.read_raw_edf(example_file, preload=True)\n",
    "preprocessed_example = preprocess(load_example)\n",
    "edf_length = get_edf_length(load_example)\n",
    "bin_width = 10000\n",
    "bin_interval = 5000\n",
    "\n",
    "num_of_bins = math.floor(((edf_length - (bin_width / 1000)) * 1000)/bin_interval) + 1\n",
    "# Save results to file\n",
    "# bin width is always a multiple of 1000, so no need to math floor\n",
    "# But do it anyway to get an int\n",
    "bin_width_s = math.floor(bin_width / 1000)\n",
    "bin_int_s = bin_interval / 1000\n",
    "# Prediction for each bin\n",
    "for i in range(num_of_bins):\n",
    "    # start of ith bin in milliseconds: i * bin_interval\n",
    "    # Of course there are 128 data points in a second\n",
    "    # So proper formula is i * bin_interval * 128/1000\n",
    "    # Round down\n",
    "    bin_start = math.floor(i * bin_int_s * 128)\n",
    "    # end of ith bin = start of bin + bin width\n",
    "    # Conversion: start of bin + (bin width * 128/1000)\n",
    "    bin_end = bin_start + (bin_width_s * 128)\n",
    "    curr_bin = preprocessed_example[0:64, bin_start:bin_end]\n",
    "    # bin_width > 1, but model expects 1 second each\n",
    "    # Try passing batch of 1 sec\n",
    "    batched_bins = np.zeros((bin_width_s, 64, 128, 1))\n",
    "    # Make bin into batch of 1 second slices\n",
    "    # Model expecting 1 second (128 samples)\n",
    "    for j in range(bin_width_s):\n",
    "        batch_start = j * 128\n",
    "        batch_end = (j + 1) * 128\n",
    "        batched_bins[j] = curr_bin[0:64, batch_start:batch_end]\n",
    "\n",
    "    bins_squeezed = batched_bins.squeeze(-1)\n",
    "    combined_samples = np.zeros((bin_width_s, 128))\n",
    "    for idx in range(bin_width_s):\n",
    "        combined_samples[idx] = bins_squeezed[idx].mean(axis=0)\n",
    "\n",
    "    combined_samples_tf = scaler.transform(combined_samples)\n",
    "    predictions = knn.predict(combined_samples_tf)\n",
    "    result = np.mean(predictions).astype(float)\n",
    "    print('bin:', i, \" result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
